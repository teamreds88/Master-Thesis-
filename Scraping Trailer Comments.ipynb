{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0f3acf-2277-442f-820f-654a5ba39830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /opt/anaconda3/lib/python3.12/site-packages (2.169.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (2.39.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (2.24.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.4.26)\n",
      "✅ Done! Fetched 10403 English pre-release comments (from 14724 total top-level comments).\n"
     ]
    }
   ],
   "source": [
    "# First, install the required package\n",
    "!pip install google-api-python-client\n",
    "\n",
    "# Now the rest of your code will work\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import time\n",
    "\n",
    "# STEP 1: Your API info\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "VIDEO_ID = \"Way9Dexny3w\"  # Dune Part 2 trailer\n",
    "RELEASE_DATE = \"2024-03-01\"\n",
    "\n",
    "# STEP 2: Set up YouTube API client\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# STEP 3: Scrape all comments (no hard 500 limit)\n",
    "comments = []\n",
    "next_page_token = None\n",
    "total_fetched = 0\n",
    "\n",
    "while True:\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=VIDEO_ID,\n",
    "        maxResults=100,\n",
    "        pageToken=next_page_token,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    for item in response['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']\n",
    "        published_date = comment['publishedAt'][:10]\n",
    "\n",
    "        # Filter: before release date\n",
    "        if published_date < RELEASE_DATE:\n",
    "            text = comment['textDisplay']\n",
    "\n",
    "            # Filter: English only\n",
    "            try:\n",
    "                lang = detect(text)\n",
    "            except LangDetectException:\n",
    "                lang = \"unknown\"\n",
    "\n",
    "            if lang == \"en\":\n",
    "                comments.append([\n",
    "                    comment['authorDisplayName'],\n",
    "                    published_date,\n",
    "                    text,\n",
    "                    comment['likeCount']\n",
    "                ])\n",
    "\n",
    "    total_fetched += len(response['items'])\n",
    "\n",
    "    # Continue if there's a next page\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    if not next_page_token:\n",
    "        break\n",
    "\n",
    "    # Pause briefly to avoid quota issues\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# STEP 4: Create DataFrame and export\n",
    "df = pd.DataFrame(comments, columns=[\"Author\", \"Date\", \"Comment\", \"Likes\"])\n",
    "df.to_csv(\"dune2_english_comments_pre_release.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Fetched {len(df)} English pre-release comments (from {total_fetched} total top-level comments).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c1b846-8e24-49cb-8c3a-2403115b6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.12/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from langdetect) (1.16.0)\n",
      "✅ Done! Fetched 10369 English pre-release comments (from 14724 total top-level comments).\n"
     ]
    }
   ],
   "source": [
    "# First, install the required package\n",
    "!pip install langdetect\n",
    "\n",
    "# Loading in the required packages \n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import time\n",
    "\n",
    "######## Initial check by scraping pre-release comments for Dune 2\n",
    "# STEP 1: API info\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "VIDEO_ID = \"Way9Dexny3w\"  # Dune Part 2 trailer\n",
    "RELEASE_DATE = \"2024-02-29\"\n",
    "\n",
    "# STEP 2: Set up YouTube API client\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# STEP 3: Scrape all comments (no hard 500 limit)\n",
    "comments = []\n",
    "next_page_token = None\n",
    "total_fetched = 0\n",
    "\n",
    "while True:\n",
    "    request = youtube.commentThreads().list(\n",
    "        part=\"snippet\",\n",
    "        videoId=VIDEO_ID,\n",
    "        maxResults=100,\n",
    "        pageToken=next_page_token,\n",
    "        textFormat=\"plainText\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    for item in response['items']:\n",
    "        comment = item['snippet']['topLevelComment']['snippet']\n",
    "        published_date = comment['publishedAt'][:10]\n",
    "\n",
    "        # Filter: before release date\n",
    "        if published_date < RELEASE_DATE:\n",
    "            text = comment['textDisplay']\n",
    "\n",
    "            # Filter: English only\n",
    "            try:\n",
    "                lang = detect(text)\n",
    "            except LangDetectException:\n",
    "                lang = \"unknown\"\n",
    "\n",
    "            if lang == \"en\":\n",
    "                comments.append([\n",
    "                    comment['authorDisplayName'],\n",
    "                    published_date,\n",
    "                    text,\n",
    "                    comment['likeCount']\n",
    "                ])\n",
    "\n",
    "    total_fetched += len(response['items'])\n",
    "\n",
    "    # Continue if there's a next page\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    if not next_page_token:\n",
    "        break\n",
    "\n",
    "    # Pause briefly to avoid quota issues\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# STEP 4: Create DataFrame and export\n",
    "df = pd.DataFrame(comments, columns=[\"Author\", \"Date\", \"Comment\", \"Likes\"])\n",
    "df.to_csv(\"dune2_english_comments_pre_release.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Fetched {len(df)} English pre-release comments (from {total_fetched} total top-level comments).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929f26ad-52ca-42b1-8d7d-ed9946bfb86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@fonziebulldog5786</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>A good continuing to the first episode and i e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@КристиянГьошев</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>Top of the year 1000% !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@jesperihalainen3132</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>I wish that Star Wars could pull something inc...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@theshepherd6930</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>just left the theatres in France on premiere d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@syamil.ibrahim</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>I wish there’ll be Part 3, Part 2 is too short...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@pradipFRCS</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>Just watched the movie. Really great movie.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@Mr-Kind-au</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>This movie is 2 hours and 46 minutes long, if ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@Castrum28</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>Saw this today... It was fun and amazing</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@philippeserre1</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>The trailer is not a lie, the movie is awesome</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@HenryR92</td>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>It’s the on location filming that gives it tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Author        Date  \\\n",
       "0    @fonziebulldog5786  2024-02-28   \n",
       "1       @КристиянГьошев  2024-02-28   \n",
       "2  @jesperihalainen3132  2024-02-28   \n",
       "3      @theshepherd6930  2024-02-28   \n",
       "4       @syamil.ibrahim  2024-02-28   \n",
       "5           @pradipFRCS  2024-02-28   \n",
       "6           @Mr-Kind-au  2024-02-28   \n",
       "7            @Castrum28  2024-02-28   \n",
       "8       @philippeserre1  2024-02-28   \n",
       "9             @HenryR92  2024-02-28   \n",
       "\n",
       "                                             Comment  Likes  \n",
       "0  A good continuing to the first episode and i e...      0  \n",
       "1                            Top of the year 1000% !      1  \n",
       "2  I wish that Star Wars could pull something inc...      4  \n",
       "3  just left the theatres in France on premiere d...      0  \n",
       "4  I wish there’ll be Part 3, Part 2 is too short...      0  \n",
       "5        Just watched the movie. Really great movie.      1  \n",
       "6  This movie is 2 hours and 46 minutes long, if ...      0  \n",
       "7           Saw this today... It was fun and amazing      5  \n",
       "8     The trailer is not a lie, the movie is awesome      0  \n",
       "9  It’s the on location filming that gives it tha...      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"dune2_english_comments_pre_release.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f84ce11-62c7-48ce-a44f-78e4ab7af030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /opt/anaconda3/lib/python3.12/site-packages (2.169.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.12/site-packages (1.0.9)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (2.39.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (2.24.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client pandas langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e57d756a-9878-462d-b2cb-cfac24e954a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf8fd9bf-7794-4d10-b031-cecf2d202f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual API key\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "\n",
    "# Connect to YouTube API\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33c6be4-46b5-4c05-91cd-c12e12882b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your working directory (replace this with your actual path)\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Now read the file\n",
    "df = pd.read_csv(\"first_10_movies_for_scraping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cab1e57b-ab64-40a5-8502-856c5822bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial scraper for video IDs of relevant YouTube trailers of 10 test titles \n",
    "def get_video_id_with_distributor(row):\n",
    "    query = f\"{row['title']} {pd.to_datetime(row['release_date']).year} official trailer {row['us_distributor']}\"\n",
    "    distributor = str(row['us_distributor']).lower()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "\n",
    "        result = response['items'][0]\n",
    "        video_id = result['id']['videoId']\n",
    "        channel_title = result['snippet']['channelTitle'].lower()\n",
    "\n",
    "        # Loose match on distributor keyword\n",
    "        if distributor.split()[0] in channel_title:\n",
    "            return video_id\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Apply to the 10 movies\n",
    "df['video_id'] = df.apply(get_video_id_with_distributor, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8e656be-bb47-4e03-8433-096ddbdf39da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release_date</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>dist</th>\n",
       "      <th>us_distributor</th>\n",
       "      <th>opening_locs</th>\n",
       "      <th>widest_locs</th>\n",
       "      <th>opening_wknd_local_currency</th>\n",
       "      <th>opening_week_local_currency</th>\n",
       "      <th>cume_gross_local_currency</th>\n",
       "      <th>...</th>\n",
       "      <th>run_time</th>\n",
       "      <th>languages_of_origin</th>\n",
       "      <th>primary_territories_of_origin</th>\n",
       "      <th>non_primary_territories_of_origin</th>\n",
       "      <th>booking_title_number</th>\n",
       "      <th>bafta_awards</th>\n",
       "      <th>title_global_id</th>\n",
       "      <th>release_year</th>\n",
       "      <th>youtube_search_query</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>12</td>\n",
       "      <td>Lusomundo</td>\n",
       "      <td>Fox Searchlight</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>63117</td>\n",
       "      <td>81323</td>\n",
       "      <td>178359</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>144982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-86352</td>\n",
       "      <td>2019</td>\n",
       "      <td>Old Man &amp; The Gun, The 2019 official trailer F...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Holmes And Watson</td>\n",
       "      <td>12</td>\n",
       "      <td>Sony Int'l</td>\n",
       "      <td>Sony</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>78753</td>\n",
       "      <td>94924</td>\n",
       "      <td>177227</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>133113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-78696</td>\n",
       "      <td>2019</td>\n",
       "      <td>Holmes And Watson 2019 official trailer Sony</td>\n",
       "      <td>brjkpRBpFnc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Kindergarten Teacher, The</td>\n",
       "      <td>14</td>\n",
       "      <td>Alambique</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>6906</td>\n",
       "      <td>12203</td>\n",
       "      <td>20121</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>150275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-91047</td>\n",
       "      <td>2019</td>\n",
       "      <td>Kindergarten Teacher, The 2019 official traile...</td>\n",
       "      <td>v6f186C-N90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Here And Now (2018)</td>\n",
       "      <td>12</td>\n",
       "      <td>Lusomundo</td>\n",
       "      <td>Independent</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5703</td>\n",
       "      <td>7803</td>\n",
       "      <td>9950</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>139867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-82272</td>\n",
       "      <td>2019</td>\n",
       "      <td>Here And Now (2018) 2019 official trailer Inde...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>True Crimes</td>\n",
       "      <td>16</td>\n",
       "      <td>Lanterna De Pedra</td>\n",
       "      <td>Independent</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1847</td>\n",
       "      <td>2612</td>\n",
       "      <td>3644</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>147594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-88245</td>\n",
       "      <td>2019</td>\n",
       "      <td>True Crimes 2019 official trailer Independent</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Escape Room</td>\n",
       "      <td>14</td>\n",
       "      <td>Sony Int'l</td>\n",
       "      <td>Sony</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "      <td>124146</td>\n",
       "      <td>159564</td>\n",
       "      <td>436569</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>147890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-89040</td>\n",
       "      <td>2019</td>\n",
       "      <td>Escape Room 2019 official trailer Sony</td>\n",
       "      <td>6dSKUoV0SNI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>On The Basis Of Sex</td>\n",
       "      <td>12</td>\n",
       "      <td>PRIS</td>\n",
       "      <td>Focus Features</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>39645</td>\n",
       "      <td>51380</td>\n",
       "      <td>88415</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>144666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-85844</td>\n",
       "      <td>2019</td>\n",
       "      <td>On The Basis Of Sex 2019 official trailer Focu...</td>\n",
       "      <td>28dHbIR_NB4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Vox Lux</td>\n",
       "      <td>14</td>\n",
       "      <td>Lusomundo</td>\n",
       "      <td>Neon Rated</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>11830</td>\n",
       "      <td>15329</td>\n",
       "      <td>19647</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>152232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-92274</td>\n",
       "      <td>2019</td>\n",
       "      <td>Vox Lux 2019 official trailer Neon Rated</td>\n",
       "      <td>zxdVqr4hmZU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Once Upon A Deadpool</td>\n",
       "      <td>12</td>\n",
       "      <td>Fox Int'l</td>\n",
       "      <td>20th Century Fox</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4357</td>\n",
       "      <td>5555</td>\n",
       "      <td>5555</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>152288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-92333</td>\n",
       "      <td>2019</td>\n",
       "      <td>Once Upon A Deadpool 2019 official trailer 20t...</td>\n",
       "      <td>PCf03KXyzIg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>Glass</td>\n",
       "      <td>14</td>\n",
       "      <td>Walt Disney Int'l</td>\n",
       "      <td>Universal</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>290426</td>\n",
       "      <td>365794</td>\n",
       "      <td>820489</td>\n",
       "      <td>...</td>\n",
       "      <td>128</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>130236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-73533</td>\n",
       "      <td>2019</td>\n",
       "      <td>Glass 2019 official trailer Universal</td>\n",
       "      <td>95ghQs5AmNk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  release_date                      title  rating               dist  \\\n",
       "0   2019-01-03     Old Man & The Gun, The      12          Lusomundo   \n",
       "1   2019-01-03          Holmes And Watson      12         Sony Int'l   \n",
       "2   2019-01-03  Kindergarten Teacher, The      14          Alambique   \n",
       "3   2019-01-03        Here And Now (2018)      12          Lusomundo   \n",
       "4   2019-01-03                True Crimes      16  Lanterna De Pedra   \n",
       "5   2019-01-10                Escape Room      14         Sony Int'l   \n",
       "6   2019-01-10        On The Basis Of Sex      12               PRIS   \n",
       "7   2019-01-10                    Vox Lux      14          Lusomundo   \n",
       "8   2019-01-10       Once Upon A Deadpool      12          Fox Int'l   \n",
       "9   2019-01-17                      Glass      14  Walt Disney Int'l   \n",
       "\n",
       "     us_distributor  opening_locs  widest_locs  opening_wknd_local_currency  \\\n",
       "0   Fox Searchlight            35           35                        63117   \n",
       "1              Sony            48           48                        78753   \n",
       "2               NaN            13           13                         6906   \n",
       "3       Independent            10           10                         5703   \n",
       "4       Independent             6            6                         1847   \n",
       "5              Sony            33           38                       124146   \n",
       "6    Focus Features            32           32                        39645   \n",
       "7        Neon Rated            18           18                        11830   \n",
       "8  20th Century Fox             7            7                         4357   \n",
       "9         Universal            83           83                       290426   \n",
       "\n",
       "   opening_week_local_currency  cume_gross_local_currency  ...  run_time  \\\n",
       "0                        81323                     178359  ...        93   \n",
       "1                        94924                     177227  ...        90   \n",
       "2                        12203                      20121  ...        96   \n",
       "3                         7803                       9950  ...        91   \n",
       "4                         2612                       3644  ...        92   \n",
       "5                       159564                     436569  ...       100   \n",
       "6                        51380                      88415  ...       120   \n",
       "7                        15329                      19647  ...       110   \n",
       "8                         5555                       5555  ...       119   \n",
       "9                       365794                     820489  ...       128   \n",
       "\n",
       "   languages_of_origin  primary_territories_of_origin  \\\n",
       "0              English                            USA   \n",
       "1              English                            USA   \n",
       "2              English                            USA   \n",
       "3              English                            USA   \n",
       "4              English                            USA   \n",
       "5              English                            USA   \n",
       "6              English                            USA   \n",
       "7              English                            USA   \n",
       "8              English                            USA   \n",
       "9              English                            USA   \n",
       "\n",
       "   non_primary_territories_of_origin  booking_title_number  bafta_awards  \\\n",
       "0                                USA                144982           NaN   \n",
       "1                                USA                133113           NaN   \n",
       "2                                USA                150275           NaN   \n",
       "3                                USA                139867           NaN   \n",
       "4                                USA                147594           NaN   \n",
       "5                                USA                147890           NaN   \n",
       "6                                USA                144666           NaN   \n",
       "7                                USA                152232           NaN   \n",
       "8                                USA                152288           NaN   \n",
       "9                                USA                130236           NaN   \n",
       "\n",
       "   title_global_id  release_year  \\\n",
       "0        TTL-86352          2019   \n",
       "1        TTL-78696          2019   \n",
       "2        TTL-91047          2019   \n",
       "3        TTL-82272          2019   \n",
       "4        TTL-88245          2019   \n",
       "5        TTL-89040          2019   \n",
       "6        TTL-85844          2019   \n",
       "7        TTL-92274          2019   \n",
       "8        TTL-92333          2019   \n",
       "9        TTL-73533          2019   \n",
       "\n",
       "                                youtube_search_query     video_id  \n",
       "0  Old Man & The Gun, The 2019 official trailer F...         None  \n",
       "1       Holmes And Watson 2019 official trailer Sony  brjkpRBpFnc  \n",
       "2  Kindergarten Teacher, The 2019 official traile...  v6f186C-N90  \n",
       "3  Here And Now (2018) 2019 official trailer Inde...         None  \n",
       "4      True Crimes 2019 official trailer Independent         None  \n",
       "5             Escape Room 2019 official trailer Sony  6dSKUoV0SNI  \n",
       "6  On The Basis Of Sex 2019 official trailer Focu...  28dHbIR_NB4  \n",
       "7           Vox Lux 2019 official trailer Neon Rated  zxdVqr4hmZU  \n",
       "8  Once Upon A Deadpool 2019 official trailer 20t...  PCf03KXyzIg  \n",
       "9              Glass 2019 official trailer Universal  95ghQs5AmNk  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)  # First 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "189e486c-75ed-42c8-a047-173b6282fd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done scraping video IDs for first 10 movies.\n"
     ]
    }
   ],
   "source": [
    "##### some titles had missing video IDs; new scraper defined with distributor alias dictionary for the 10 test titles\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load data\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")  # adjust this if needed\n",
    "df = pd.read_csv(\"first_10_movies_for_scraping.csv\")\n",
    "\n",
    "# YouTube API setup\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor alias dictionary\n",
    "distributor_aliases = {\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"warner bros.\": \"warnerbros\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "}\n",
    "\n",
    "# Function to build query and get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply to first 10\n",
    "df['video_id'] = df.apply(get_video_id, axis=1)\n",
    "\n",
    "# Save result\n",
    "df.to_csv(\"first_10_movies_with_video_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for first 10 movies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6327d67-ef92-425f-94f0-3dc1e46daf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>release_date</th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>dist</th>\n",
       "      <th>us_distributor</th>\n",
       "      <th>opening_locs</th>\n",
       "      <th>widest_locs</th>\n",
       "      <th>opening_wknd_local_currency</th>\n",
       "      <th>opening_week_local_currency</th>\n",
       "      <th>cume_gross_local_currency</th>\n",
       "      <th>...</th>\n",
       "      <th>run_time</th>\n",
       "      <th>languages_of_origin</th>\n",
       "      <th>primary_territories_of_origin</th>\n",
       "      <th>non_primary_territories_of_origin</th>\n",
       "      <th>booking_title_number</th>\n",
       "      <th>bafta_awards</th>\n",
       "      <th>title_global_id</th>\n",
       "      <th>release_year</th>\n",
       "      <th>youtube_search_query</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>12</td>\n",
       "      <td>Lusomundo</td>\n",
       "      <td>Fox Searchlight</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>63117</td>\n",
       "      <td>81323</td>\n",
       "      <td>178359</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>144982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-86352</td>\n",
       "      <td>2019</td>\n",
       "      <td>Old Man &amp; The Gun, The 2019 official trailer F...</td>\n",
       "      <td>d7rlUe-Thvk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Holmes And Watson</td>\n",
       "      <td>12</td>\n",
       "      <td>Sony Int'l</td>\n",
       "      <td>Sony</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>78753</td>\n",
       "      <td>94924</td>\n",
       "      <td>177227</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>133113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-78696</td>\n",
       "      <td>2019</td>\n",
       "      <td>Holmes And Watson 2019 official trailer Sony</td>\n",
       "      <td>brjkpRBpFnc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Kindergarten Teacher, The</td>\n",
       "      <td>14</td>\n",
       "      <td>Alambique</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>6906</td>\n",
       "      <td>12203</td>\n",
       "      <td>20121</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>150275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-91047</td>\n",
       "      <td>2019</td>\n",
       "      <td>Kindergarten Teacher, The 2019 official traile...</td>\n",
       "      <td>S7Tpamr2P4w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Here And Now (2018)</td>\n",
       "      <td>12</td>\n",
       "      <td>Lusomundo</td>\n",
       "      <td>Independent</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5703</td>\n",
       "      <td>7803</td>\n",
       "      <td>9950</td>\n",
       "      <td>...</td>\n",
       "      <td>91</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>139867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-82272</td>\n",
       "      <td>2019</td>\n",
       "      <td>Here And Now (2018) 2019 official trailer Inde...</td>\n",
       "      <td>TOWhOVx8IMI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>True Crimes</td>\n",
       "      <td>16</td>\n",
       "      <td>Lanterna De Pedra</td>\n",
       "      <td>Independent</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1847</td>\n",
       "      <td>2612</td>\n",
       "      <td>3644</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>147594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-88245</td>\n",
       "      <td>2019</td>\n",
       "      <td>True Crimes 2019 official trailer Independent</td>\n",
       "      <td>QEijgVViWpw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Escape Room</td>\n",
       "      <td>14</td>\n",
       "      <td>Sony Int'l</td>\n",
       "      <td>Sony</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "      <td>124146</td>\n",
       "      <td>159564</td>\n",
       "      <td>436569</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>147890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-89040</td>\n",
       "      <td>2019</td>\n",
       "      <td>Escape Room 2019 official trailer Sony</td>\n",
       "      <td>6dSKUoV0SNI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>On The Basis Of Sex</td>\n",
       "      <td>12</td>\n",
       "      <td>PRIS</td>\n",
       "      <td>Focus Features</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>39645</td>\n",
       "      <td>51380</td>\n",
       "      <td>88415</td>\n",
       "      <td>...</td>\n",
       "      <td>120</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>144666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-85844</td>\n",
       "      <td>2019</td>\n",
       "      <td>On The Basis Of Sex 2019 official trailer Focu...</td>\n",
       "      <td>28dHbIR_NB4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Vox Lux</td>\n",
       "      <td>14</td>\n",
       "      <td>Lusomundo</td>\n",
       "      <td>Neon Rated</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>11830</td>\n",
       "      <td>15329</td>\n",
       "      <td>19647</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>152232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-92274</td>\n",
       "      <td>2019</td>\n",
       "      <td>Vox Lux 2019 official trailer Neon Rated</td>\n",
       "      <td>-y4ti900jog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-01-10</td>\n",
       "      <td>Once Upon A Deadpool</td>\n",
       "      <td>12</td>\n",
       "      <td>Fox Int'l</td>\n",
       "      <td>20th Century Fox</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4357</td>\n",
       "      <td>5555</td>\n",
       "      <td>5555</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>152288</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-92333</td>\n",
       "      <td>2019</td>\n",
       "      <td>Once Upon A Deadpool 2019 official trailer 20t...</td>\n",
       "      <td>PCf03KXyzIg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-01-17</td>\n",
       "      <td>Glass</td>\n",
       "      <td>14</td>\n",
       "      <td>Walt Disney Int'l</td>\n",
       "      <td>Universal</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>290426</td>\n",
       "      <td>365794</td>\n",
       "      <td>820489</td>\n",
       "      <td>...</td>\n",
       "      <td>128</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>130236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TTL-73533</td>\n",
       "      <td>2019</td>\n",
       "      <td>Glass 2019 official trailer Universal</td>\n",
       "      <td>95ghQs5AmNk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  release_date                      title  rating               dist  \\\n",
       "0   2019-01-03     Old Man & The Gun, The      12          Lusomundo   \n",
       "1   2019-01-03          Holmes And Watson      12         Sony Int'l   \n",
       "2   2019-01-03  Kindergarten Teacher, The      14          Alambique   \n",
       "3   2019-01-03        Here And Now (2018)      12          Lusomundo   \n",
       "4   2019-01-03                True Crimes      16  Lanterna De Pedra   \n",
       "5   2019-01-10                Escape Room      14         Sony Int'l   \n",
       "6   2019-01-10        On The Basis Of Sex      12               PRIS   \n",
       "7   2019-01-10                    Vox Lux      14          Lusomundo   \n",
       "8   2019-01-10       Once Upon A Deadpool      12          Fox Int'l   \n",
       "9   2019-01-17                      Glass      14  Walt Disney Int'l   \n",
       "\n",
       "     us_distributor  opening_locs  widest_locs  opening_wknd_local_currency  \\\n",
       "0   Fox Searchlight            35           35                        63117   \n",
       "1              Sony            48           48                        78753   \n",
       "2               NaN            13           13                         6906   \n",
       "3       Independent            10           10                         5703   \n",
       "4       Independent             6            6                         1847   \n",
       "5              Sony            33           38                       124146   \n",
       "6    Focus Features            32           32                        39645   \n",
       "7        Neon Rated            18           18                        11830   \n",
       "8  20th Century Fox             7            7                         4357   \n",
       "9         Universal            83           83                       290426   \n",
       "\n",
       "   opening_week_local_currency  cume_gross_local_currency  ...  run_time  \\\n",
       "0                        81323                     178359  ...        93   \n",
       "1                        94924                     177227  ...        90   \n",
       "2                        12203                      20121  ...        96   \n",
       "3                         7803                       9950  ...        91   \n",
       "4                         2612                       3644  ...        92   \n",
       "5                       159564                     436569  ...       100   \n",
       "6                        51380                      88415  ...       120   \n",
       "7                        15329                      19647  ...       110   \n",
       "8                         5555                       5555  ...       119   \n",
       "9                       365794                     820489  ...       128   \n",
       "\n",
       "   languages_of_origin  primary_territories_of_origin  \\\n",
       "0              English                            USA   \n",
       "1              English                            USA   \n",
       "2              English                            USA   \n",
       "3              English                            USA   \n",
       "4              English                            USA   \n",
       "5              English                            USA   \n",
       "6              English                            USA   \n",
       "7              English                            USA   \n",
       "8              English                            USA   \n",
       "9              English                            USA   \n",
       "\n",
       "   non_primary_territories_of_origin  booking_title_number  bafta_awards  \\\n",
       "0                                USA                144982           NaN   \n",
       "1                                USA                133113           NaN   \n",
       "2                                USA                150275           NaN   \n",
       "3                                USA                139867           NaN   \n",
       "4                                USA                147594           NaN   \n",
       "5                                USA                147890           NaN   \n",
       "6                                USA                144666           NaN   \n",
       "7                                USA                152232           NaN   \n",
       "8                                USA                152288           NaN   \n",
       "9                                USA                130236           NaN   \n",
       "\n",
       "   title_global_id  release_year  \\\n",
       "0        TTL-86352          2019   \n",
       "1        TTL-78696          2019   \n",
       "2        TTL-91047          2019   \n",
       "3        TTL-82272          2019   \n",
       "4        TTL-88245          2019   \n",
       "5        TTL-89040          2019   \n",
       "6        TTL-85844          2019   \n",
       "7        TTL-92274          2019   \n",
       "8        TTL-92333          2019   \n",
       "9        TTL-73533          2019   \n",
       "\n",
       "                                youtube_search_query     video_id  \n",
       "0  Old Man & The Gun, The 2019 official trailer F...  d7rlUe-Thvk  \n",
       "1       Holmes And Watson 2019 official trailer Sony  brjkpRBpFnc  \n",
       "2  Kindergarten Teacher, The 2019 official traile...  S7Tpamr2P4w  \n",
       "3  Here And Now (2018) 2019 official trailer Inde...  TOWhOVx8IMI  \n",
       "4      True Crimes 2019 official trailer Independent  QEijgVViWpw  \n",
       "5             Escape Room 2019 official trailer Sony  6dSKUoV0SNI  \n",
       "6  On The Basis Of Sex 2019 official trailer Focu...  28dHbIR_NB4  \n",
       "7           Vox Lux 2019 official trailer Neon Rated  -y4ti900jog  \n",
       "8  Once Upon A Deadpool 2019 official trailer 20t...  PCf03KXyzIg  \n",
       "9              Glass 2019 official trailer Universal  95ghQs5AmNk  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)  # First 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "abe7d476-66db-442e-93bd-57de2f0eefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Scraping pre-release english comments for video IDs fetched above for the 10 test titles\n",
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Setup\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Load data\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "df = pd.read_csv(\"first_10_movies_with_video_ids.csv\")\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "\n",
    "# Function to scrape comments for one video\n",
    "def scrape_comments(video_id, release_date, movie_title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    total_fetched = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"⛔️ Error fetching comments for {movie_title}: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            published_date = pd.to_datetime(comment['publishedAt']).date()\n",
    "\n",
    "            # Filter: pre-release\n",
    "            if published_date < release_date.date():\n",
    "                text = comment['textDisplay']\n",
    "                try:\n",
    "                    if detect(text) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"movie\": movie_title,\n",
    "                            \"date\": published_date,\n",
    "                            \"author\": comment['authorDisplayName'],\n",
    "                            \"comment\": text,\n",
    "                            \"likes\": comment['likeCount']\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        total_fetched += len(response['items'])\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    print(f\"✅ {movie_title}: {len(comments)} pre-release English comments from {total_fetched} total.\")\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b32807a7-93e0-4abf-a25b-9cdecfae938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Old Man & The Gun, The: 324 pre-release English comments from 518 total.\n",
      "✅ Holmes And Watson: 3966 pre-release English comments from 5558 total.\n",
      "✅ Kindergarten Teacher, The: 2 pre-release English comments from 82 total.\n",
      "✅ Here And Now (2018): 26 pre-release English comments from 62 total.\n",
      "✅ True Crimes: 2 pre-release English comments from 27 total.\n",
      "✅ Escape Room: 2023 pre-release English comments from 5476 total.\n",
      "✅ On The Basis Of Sex: 613 pre-release English comments from 1192 total.\n",
      "✅ Vox Lux: 54 pre-release English comments from 71 total.\n",
      "✅ Once Upon A Deadpool: 9834 pre-release English comments from 13604 total.\n",
      "✅ Glass: 7347 pre-release English comments from 11843 total.\n",
      "🎉 Done scraping all 10 movies. Comments saved.\n"
     ]
    }
   ],
   "source": [
    "all_comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments.extend(comments)\n",
    "    else:\n",
    "        print(f\"⚠️ No video ID for {row['title']}\")\n",
    "\n",
    "# Create and export dataframe\n",
    "comments_df = pd.DataFrame(all_comments)\n",
    "comments_df.to_csv(\"first_10_movies_comments.csv\", index=False)\n",
    "\n",
    "print(\"🎉 Done scraping all 10 movies. Comments saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d1c54ae-b877-42a5-9f48-6465d6dab173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie\n",
       "Once Upon A Deadpool         9834\n",
       "Glass                        7347\n",
       "Holmes And Watson            3966\n",
       "Escape Room                  2023\n",
       "On The Basis Of Sex           613\n",
       "Old Man & The Gun, The        324\n",
       "Vox Lux                        54\n",
       "Here And Now (2018)            26\n",
       "Kindergarten Teacher, The       2\n",
       "True Crimes                     2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV\n",
    "scraped_df = pd.read_csv(\"first_10_movies_comments.csv\")\n",
    "\n",
    "# View first few rows\n",
    "scraped_df.head(10)\n",
    "\n",
    "# Checking how many comments per title\n",
    "scraped_df['movie'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ef55a4c-9f52-4820-8678-c57889202108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>@michaelroby2580</td>\n",
       "      <td>Old farts movie!! :))</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>@LukeLovesRose</td>\n",
       "      <td>This is actually a very good movie. Underrated...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>@Isahiyella</td>\n",
       "      <td>Nice to see Jackson C Frank getting his music ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>@jackscott4829</td>\n",
       "      <td>Such a wonderful movie. Redford is amazing as ...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>@jeffreymoscardelli8428</td>\n",
       "      <td>Verry good movie I'll take this over a cgi bul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>@malcolmjames7388</td>\n",
       "      <td>Sorry every time I see Robert Redford in a mov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>@molonlabe459</td>\n",
       "      <td>80 year old men that dye their hair look like ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>@boutthalifefukamerikkka7623</td>\n",
       "      <td>These shit looks good af</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>@davidleegoth</td>\n",
       "      <td>Its ok</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>@chaosinorderrr</td>\n",
       "      <td>Good to see Jackson C Frank's music being feat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    movie        date                        author  \\\n",
       "0  Old Man & The Gun, The  2019-01-02              @michaelroby2580   \n",
       "1  Old Man & The Gun, The  2019-01-02                @LukeLovesRose   \n",
       "2  Old Man & The Gun, The  2019-01-02                   @Isahiyella   \n",
       "3  Old Man & The Gun, The  2019-01-02                @jackscott4829   \n",
       "4  Old Man & The Gun, The  2019-01-02       @jeffreymoscardelli8428   \n",
       "5  Old Man & The Gun, The  2019-01-01             @malcolmjames7388   \n",
       "6  Old Man & The Gun, The  2019-01-01                 @molonlabe459   \n",
       "7  Old Man & The Gun, The  2019-01-01  @boutthalifefukamerikkka7623   \n",
       "8  Old Man & The Gun, The  2019-01-01                 @davidleegoth   \n",
       "9  Old Man & The Gun, The  2018-12-31               @chaosinorderrr   \n",
       "\n",
       "                                             comment  likes  \n",
       "0                              Old farts movie!! :))      0  \n",
       "1  This is actually a very good movie. Underrated...      2  \n",
       "2  Nice to see Jackson C Frank getting his music ...      0  \n",
       "3  Such a wonderful movie. Redford is amazing as ...     23  \n",
       "4  Verry good movie I'll take this over a cgi bul...      0  \n",
       "5  Sorry every time I see Robert Redford in a mov...      0  \n",
       "6  80 year old men that dye their hair look like ...      0  \n",
       "7                           These shit looks good af      0  \n",
       "8                                             Its ok      0  \n",
       "9  Good to see Jackson C Frank's music being feat...      3  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first few rows\n",
    "scraped_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa92e5ee-2ca7-499a-be25-17304ec78c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Scraping pre-release comments for the first batch of titles (50 titles per batch)\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect, LangDetectException\n",
    "import time\n",
    "\n",
    "# Set your API Key\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Load batch from R export\n",
    "df = pd.read_csv(\"batch_1_scrape.csv\", parse_dates=[\"release_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04da70de-3ebd-4aaa-94d3-03d07437781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known YouTube channel aliases for better search results\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fcdabcd-1eff-4537-84c4-1b4e07eeb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve video IDs \n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error for '{title}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b17cff6a-6776-43ef-b7b9-cad75d6fff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done saving video IDs for batch 1.\n"
     ]
    }
   ],
   "source": [
    "df['video_id'] = df.apply(get_video_id, axis=1)\n",
    "\n",
    "# Save results for inspection\n",
    "df.to_csv(\"batch_1_with_ids.csv\", index=False)\n",
    "print(\"✅ Done saving video IDs for batch 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6f1f046-e37b-44a9-9c4e-11d0a09ac0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>us_distributor</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Old Man &amp; The Gun, The</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Fox Searchlight</td>\n",
       "      <td>d7rlUe-Thvk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Holmes And Watson</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Sony</td>\n",
       "      <td>brjkpRBpFnc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kindergarten Teacher, The</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S7Tpamr2P4w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Here And Now (2018)</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Independent</td>\n",
       "      <td>TOWhOVx8IMI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True Crimes</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Independent</td>\n",
       "      <td>QEijgVViWpw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title release_date   us_distributor     video_id\n",
       "0     Old Man & The Gun, The   2019-01-03  Fox Searchlight  d7rlUe-Thvk\n",
       "1          Holmes And Watson   2019-01-03             Sony  brjkpRBpFnc\n",
       "2  Kindergarten Teacher, The   2019-01-03              NaN  S7Tpamr2P4w\n",
       "3        Here And Now (2018)   2019-01-03      Independent  TOWhOVx8IMI\n",
       "4                True Crimes   2019-01-03      Independent  QEijgVViWpw"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c60aca7-eb4e-4b1f-99a7-7462db30a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape comments for batch 1\n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "973e6975-09f8-46a2-af20-989508a664a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Old Man & The Gun, The\n",
      "Scraping: Holmes And Watson\n",
      "Scraping: Kindergarten Teacher, The\n",
      "Scraping: Here And Now (2018)\n",
      "Scraping: True Crimes\n",
      "Scraping: Escape Room\n",
      "Scraping: On The Basis Of Sex\n",
      "Scraping: Vox Lux\n",
      "Scraping: Once Upon A Deadpool\n",
      "Scraping: Glass\n",
      "Scraping: Green Book\n",
      "Scraping: Instant Family\n",
      "Scraping: Serenity\n",
      "Scraping: Mule, The\n",
      "Scraping: Upside, The\n",
      "Scraping: At Eternity's Gate\n",
      "Scraping: Destroyer\n",
      "Scraping: Cold Pursuit\n",
      "Scraping: Favourite, The\n",
      "Scraping: Possession Of Hannah Grace, The\n",
      "Scraping: Alita: Battle Angel\n",
      "Scraping: Vice\n",
      "Scraping: Happy Death Day 2U\n",
      "Scraping: How To Train Your Dragon: The Hidden World\n",
      "Scraping: If Beale Street Could Talk\n",
      "Scraping: Sisters Brothers, The\n",
      "Scraping: Dead Trigger\n",
      "Scraping: Monrovia, Indiana\n",
      "Scraping: Lego Movie 2: The Second Part, The\n",
      "Scraping: Prodigy, The\n",
      "Scraping: Backtrace\n",
      "Scraping: Captain Marvel\n",
      "Scraping: Drunk Parents\n",
      "Scraping: Replicas\n",
      "Scraping: Us (2019)\n",
      "Scraping: Dumplin\n",
      "Scraping: Kid Like Jake, A\n",
      "Scraping: Dumbo\n",
      "Scraping: Triple Threat (2019)\n",
      "Scraping: Destination Wedding\n",
      "Scraping: Captive State\n",
      "Scraping: Shazam!\n",
      "Scraping: Pet Sematary\n",
      "Scraping: Piercing\n",
      "Scraping: After (2019)\n",
      "Scraping: Wonder Park\n",
      "Scraping: Hellboy\n",
      "Scraping: Greta (2019)\n",
      "Scraping: Curse Of La Llorona, The\n",
      "Scraping: Aftermath, The\n"
     ]
    }
   ],
   "source": [
    "all_comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93a4007f-4a64-4f78-a79c-f6bfeb9a3c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 109875 comments for batch 1.\n"
     ]
    }
   ],
   "source": [
    "comments_df = pd.DataFrame(all_comments)\n",
    "comments_df.to_csv(\"comments_batch_1.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df)} comments for batch 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "543d8d8d-7e8c-42fc-b078-4ca2dc544145",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Scraping pre-release comments for the 2nd batch of titles (50 titles per batch\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect, LangDetectException\n",
    "import time\n",
    "\n",
    "# API Setup\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Load batch\n",
    "df = pd.read_csv(\"batch_2_scrape.csv\", parse_dates=[\"release_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5eedd0c-f691-4617-acdc-6d45e9e00a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve video IDs for batch 2\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error for '{title}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf6a9418-9eee-4e10-830a-d0e26eacea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['video_id'] = df.apply(get_video_id, axis=1)\n",
    "df.to_csv(\"batch_2_with_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad3aa751-4f9a-4a17-a5e6-41df48acb987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>us_distributor</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Missing Link</td>\n",
       "      <td>2019-04-18</td>\n",
       "      <td>United Artists Releasing</td>\n",
       "      <td>7UalKq-DSXI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avengers: Endgame</td>\n",
       "      <td>2019-04-25</td>\n",
       "      <td>Disney</td>\n",
       "      <td>-9hhb3M6c1A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Teen Spirit</td>\n",
       "      <td>2019-04-25</td>\n",
       "      <td>Bleecker Street</td>\n",
       "      <td>CkA6DpQEKTU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Silencio de los Otros, El</td>\n",
       "      <td>2019-04-25</td>\n",
       "      <td>Argot Pictures</td>\n",
       "      <td>i3ZEb8A6VKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sea Of Trees</td>\n",
       "      <td>2019-04-27</td>\n",
       "      <td>A24</td>\n",
       "      <td>7RsImMNbvNk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title release_date            us_distributor  \\\n",
       "0               Missing Link   2019-04-18  United Artists Releasing   \n",
       "1          Avengers: Endgame   2019-04-25                    Disney   \n",
       "2                Teen Spirit   2019-04-25           Bleecker Street   \n",
       "3  Silencio de los Otros, El   2019-04-25            Argot Pictures   \n",
       "4               Sea Of Trees   2019-04-27                       A24   \n",
       "\n",
       "      video_id  \n",
       "0  7UalKq-DSXI  \n",
       "1  -9hhb3M6c1A  \n",
       "2  CkA6DpQEKTU  \n",
       "3  i3ZEb8A6VKE  \n",
       "4  7RsImMNbvNk  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c79bd9e5-d8db-4c88-b229-fe7fef9317d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect, LangDetectException\n",
    "import time\n",
    "\n",
    "# Replace with your actual API key\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Load batch 2 dataset with video IDs\n",
    "df = pd.read_csv(\"batch_2_with_ids.csv\", parse_dates=[\"release_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6555f8a-fb75-43f8-be3c-b58424401898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the comment scraper \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break  # Stop if there's an API error (e.g., quota exhausted)\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98ae1207-9f12-4b84-aa7f-100af880b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Missing Link\n",
      "Scraping: Avengers: Endgame\n",
      "Scraping: Teen Spirit\n",
      "Scraping: Silencio de los Otros, El\n",
      "Scraping: Sea Of Trees\n",
      "Scraping: Long Shot (2019)\n",
      "Scraping: Pokemon Detective Pikachu\n",
      "Scraping: Breakthrough (2019)\n",
      "Scraping: Intruder, The (2019)\n",
      "Scraping: We Die Young\n",
      "Scraping: Beach Bum, The\n",
      "Scraping: John Wick: Chapter 3 - Parabellum (2019)\n",
      "Scraping: Fighting With My Family\n",
      "Scraping: Extremely Wicked, Shockingly Evil and Vile\n",
      "Scraping: Dog's Journey, A\n",
      "Scraping: Aladdin\n",
      "Scraping: Brightburn\n",
      "Scraping: Kid, The\n",
      "Scraping: Godzilla: King Of The Monsters (2019)\n",
      "Scraping: Rocketman\n",
      "Scraping: Poison Rose, The\n",
      "Scraping: Haunting Of Sharon Tate, The\n",
      "Scraping: Rape of Recy Taylor, The\n",
      "Scraping: Don't Let Go\n",
      "Scraping: Secret Life Of Pets 2, The\n",
      "Scraping: Dark Phoenix\n",
      "Scraping: Men In Black International\n",
      "Scraping: Poms\n",
      "Scraping: Eyes Of Orson Welles, The\n",
      "Scraping: Hustle, The\n",
      "Scraping: Dead Don't Die, The\n",
      "Scraping: Pavarotti\n",
      "Scraping: Vigilante, A\n",
      "Scraping: Toy Story 4\n",
      "Scraping: Annabelle Comes Home\n",
      "Scraping: Soundgarden: Live From The Artists Den - The IMAX Experience\n",
      "Scraping: Spider-Man: Far From Home\n",
      "Scraping: Escape Plan 3: Devil's Station\n",
      "Scraping: Sun Is Also A Star, The\n",
      "Scraping: Stuber\n",
      "Scraping: Crawl\n",
      "Scraping: Dragged Across Concrete\n",
      "Scraping: Her Smell\n",
      "Scraping: Lion King, The\n",
      "Scraping: Child's Play\n",
      "Scraping: Fast & Furious Presents: Hobbs & Shaw\n",
      "Scraping: Uglydolls\n",
      "Scraping: Professor, The\n",
      "Scraping: Scary Stories To Tell In The Dark\n",
      "Scraping: Kitchen, The (2019)\n"
     ]
    }
   ],
   "source": [
    "all_comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f3c33-2d33-4f1a-8950-9db7df9cbccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.DataFrame(all_comments)\n",
    "comments_df.to_csv(\"comments_batch_2.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df)} comments for batch 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ca7537d-b200-4ce2-86e2-e001e0461ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /opt/anaconda3/lib/python3.12/site-packages (2.169.0)\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.12/site-packages (1.0.9)\n",
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.13.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (2.39.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (2.24.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.4.26)\n",
      "Downloading rapidfuzz-3.13.0-cp312-cp312-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client langdetect rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5538c67-043b-44ae-80c5-11f985fc6fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Stockholm (2019) → Hna6_rWLXAQ\n",
      "✅ Found: Photograph → LwOGIlpPqek\n",
      "✅ Found: Malicious (2018) → OWCXSq61iMI\n",
      "✅ Found: Once Upon A Time In Hollywood → ELeMaP8EPAA\n",
      "✅ Found: Angry Birds Movie 2, The → RSKQ-lVsMdg\n",
      "✅ Found: Booksmart → S0Mb6BgnhS0\n",
      "✅ Found: Nomis → Rh7ZsUVCrpM\n",
      "✅ Found: Angel Has Fallen → XF8h3hOGBJM\n",
      "✅ Found: Ready Or Not → ZtYTwUxhAoI\n",
      "✅ Found: Good Boys → zPXqwAGmX04\n",
      "✅ Found: 47 Meters Down: Uncaged → zF3lM_EPh2U\n",
      "✅ Found: Art Of Racing In The Rain, The → Dp2ufFO4QGg\n",
      "✅ Found: Peanut Butter Falcon, The → UNl9RqjLCwc\n",
      "✅ Found: IT Chapter Two → xhJ5P7Up3jA\n",
      "✅ Found: Killerman → i5niXDUtGEM\n",
      "✅ Found: Goldfinch, The → IcG06hZooHM\n",
      "✅ Found: Amazing Grace → q9jPDXghVHQ\n",
      "✅ Found: Ad Astra → P6AaSMfXHbA\n",
      "✅ Found: Charlie Says → KPSKkxTQFEU\n",
      "✅ Found: Rambo: Last Blood → 4vWg5yJuWfs\n",
      "✅ Found: Hustlers → P_dfc0iqmig\n",
      "✅ Found: Midsommar → 1Vnghdsjmd0\n",
      "✅ Found: Roger Waters  Us + Them → ppisCJa7Mnc\n",
      "✅ Found: Joker → zAGVQLHvwOY\n",
      "✅ Found: Playmobil: The Movie → xgyP9GG9Ecw\n",
      "✅ Found: Skin → a3CGU9pqMQU\n",
      "✅ Found: Metallica & San Francisco Symphony: S&M2 → yRCLe6_kBRc\n",
      "✅ Found: Gemini Man → AbyJignbSj0\n",
      "✅ Found: Current War, The → kue18AxK1tU\n",
      "✅ Found: Driven → WK82Y6TKHg8\n",
      "✅ Found: Maleficent: Mistress Of Evil → yL1f8yNxGBk\n",
      "✅ Found: Abominable (2019) → OF5mVdYNutI\n",
      "✅ Found: After The Wedding → 22pa0wpGa9Q\n",
      "✅ Found: 10 Minutes Gone → 6TNFcyEN-3w\n",
      "✅ Found: Kill Team, The (2013) → -RQb2z0oS10\n",
      "✅ Found: Mutant Blast → al2ZhJzK1FU\n",
      "✅ Found: Rainy Day in New York, A → yIVRldiVDL8\n",
      "✅ Found: Zombieland: Double Tap → ZlW9yhUKlkQ\n",
      "✅ Found: Angel Of Mine → G1BD8EgwmPA\n",
      "✅ Found: Apollo 11 → 3Co8Z8BQgWc\n",
      "✅ Found: Jesus Is King → 0xJrWOrCl80\n",
      "✅ Found: Terminator: Dark Fate → oxy8udgWRmo\n",
      "✅ Found: Addams Family, The → c95edsw17QE\n",
      "✅ Found: Doctor Sleep → BOzFZxB-8cw\n",
      "✅ Found: Wanda (1970) → Fl0PETEBBhY\n",
      "✅ Found: Midway → l9laReRAYFk\n",
      "✅ Found: Playing With Fire → fd5GlZUpfaM\n",
      "✅ Found: Countdown (2019) → S6O4iy3Twwo\n",
      "✅ Found: Light Of My Life → PoHADU7Oe-g\n",
      "✅ Found: League Of Legends World Championship 2019 → i1IKnWDecwA\n",
      "✅ Done scraping video IDs for batch 3.\n"
     ]
    }
   ],
   "source": [
    "############ scraping comments for batch 3\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set your working directory to where batch_3.csv is located\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_3_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Known YouTube channel aliases for distributors\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    # Build query\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "df['video_id'] = df.apply(get_video_id, axis=1)\n",
    "\n",
    "# Save to new CSV\n",
    "df.to_csv(\"batch_3_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef367a6d-1072-423b-aee6-de63a870d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f73d235-5b3f-45cf-aa35-345f1a4bb1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_3_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7802089b-4c57-4695-b4b2-f83c98668c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # use your actual API key again\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7fdc30f-d2a7-4217-b9cb-f6db225b6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5f154cd-db06-48df-a470-36aa341edcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Stockholm (2019)\n",
      "Scraping: Photograph\n",
      "Scraping: Malicious (2018)\n",
      "Scraping: Once Upon A Time In Hollywood\n",
      "Scraping: Angry Birds Movie 2, The\n",
      "Scraping: Booksmart\n",
      "Scraping: Nomis\n",
      "Scraping: Angel Has Fallen\n",
      "Scraping: Ready Or Not\n",
      "Scraping: Good Boys\n",
      "Scraping: 47 Meters Down: Uncaged\n",
      "Scraping: Art Of Racing In The Rain, The\n",
      "Scraping: Peanut Butter Falcon, The\n",
      "Scraping: IT Chapter Two\n",
      "Scraping: Killerman\n",
      "Scraping: Goldfinch, The\n",
      "Scraping: Amazing Grace\n",
      "Scraping: Ad Astra\n",
      "Scraping: Charlie Says\n",
      "Scraping: Rambo: Last Blood\n",
      "Scraping: Hustlers\n",
      "Scraping: Midsommar\n",
      "Scraping: Roger Waters  Us + Them\n",
      "Scraping: Joker\n",
      "Scraping: Playmobil: The Movie\n",
      "Scraping: Skin\n",
      "Scraping: Metallica & San Francisco Symphony: S&M2\n",
      "Scraping: Gemini Man\n",
      "Scraping: Current War, The\n",
      "Scraping: Driven\n",
      "Scraping: Maleficent: Mistress Of Evil\n",
      "Scraping: Abominable (2019)\n",
      "Scraping: After The Wedding\n",
      "Scraping: 10 Minutes Gone\n",
      "Scraping: Kill Team, The (2013)\n",
      "Scraping: Mutant Blast\n",
      "Scraping: Rainy Day in New York, A\n",
      "Scraping: Zombieland: Double Tap\n",
      "Scraping: Angel Of Mine\n",
      "Scraping: Apollo 11\n",
      "Scraping: Jesus Is King\n",
      "Scraping: Terminator: Dark Fate\n",
      "Scraping: Addams Family, The\n",
      "Scraping: Doctor Sleep\n",
      "Scraping: Wanda (1970)\n",
      "Scraping: Midway\n",
      "Scraping: Playing With Fire\n",
      "Scraping: Countdown (2019)\n",
      "Scraping: Light Of My Life\n",
      "Scraping: League Of Legends World Championship 2019\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch3 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch3.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e58bacb-7902-4039-947a-fb7482c5afd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 122632 comments for batch 3.\n"
     ]
    }
   ],
   "source": [
    "comments_df_3 = pd.DataFrame(all_comments_batch3)\n",
    "comments_df_3.to_csv(\"comments_batch_3.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_3)} comments for batch 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adb4a475-6084-46d6-a4af-de8b051cdcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Shakira In Concert: El Dorado World Tour → 8og9uOzDdiY\n",
      "✅ Found: Ford v. Ferrari → zyYgDtY2AMY\n",
      "✅ Found: Motherless Brooklyn → Fru8IkuDp_k\n",
      "✅ Found: Frozen 2 (2019) → Zi4LMpSDccc\n",
      "✅ Found: Depeche Mode: Spirits In The Forest → 03rZB3AsLqM\n",
      "✅ Found: MET Opera: Akhnaten (2019) → rSn_UAquOfw\n",
      "✅ Found: Knives Out → qGqiHJTsRkQ\n",
      "✅ Found: Charlie's Angels → RSUq4VfWfjE\n",
      "✅ Found: Where'd You Go Bernadette → Em1onUCArxs\n",
      "✅ Found: Last Christmas → z9CEIcmWmtA\n",
      "✅ Found: Aeronauts, The → Rm4VnwCtQO8\n",
      "✅ Found: Jumanji: The Next Level → rBxcF-r9Ibs\n",
      "✅ Found: 21 Bridges → BVZDhunTrYA\n",
      "✅ Found: Star Wars: The Rise Of Skywalker → NmS9ZX9O5bw\n",
      "✅ Found: Can You Keep A Secret? → 3IM1vSNJw3Y\n",
      "✅ Found: Apocalypse Now Final Cut (2019) → 036ZoFx1SVI\n",
      "✅ Found: Murder Of Nicole Brown Simpson, The → txkFJoFQjAE\n",
      "✅ Found: Spies In Disguise → 9eY2W7uUkDE\n",
      "✅ Found: Jexi → 8UqXUwgl15k\n",
      "✅ Found: Cats → FtSd844cI7U\n",
      "✅ Found: Grudge, The → O2NKzO-fxwQ\n",
      "✅ Found: Richard Jewell → gSMxBLlA8qY\n",
      "✅ Found: Plus One → RZSeK851vZY\n",
      "✅ Found: Beyond the mountains and hills → yT4ZTwCjjGU\n",
      "✅ Found: Dark Waters → RvAOuhyunhY\n",
      "✅ Found: Primal → MLSaNBuKOXk\n",
      "✅ Found: Farewell, The → RofpAjqwMa8\n",
      "✅ Found: Bad Boys For Life → jKCj3XuPG8M\n",
      "✅ Found: Dolittle → FEf412bSPLs\n",
      "✅ Found: Just Mercy → GVQbeG5yW78\n",
      "✅ Found: 1917 → YqNYrYUiMfg\n",
      "✅ Found: Bombshell → 0rBnkBIhoFE\n",
      "✅ Found: Disturbing The Peace → NVYPYwHYeyc\n",
      "✅ Found: Wedding Year, The → qp9sbE8-1y8\n",
      "✅ Found: Little Women → AST2-4db4ic\n",
      "✅ Found: Jojo Rabbit → tL4McUzXfFI\n",
      "✅ Found: Turning, The → rl33gU2APIs\n",
      "✅ Found: Birds Of Prey (And The Fantabulous Emancipation Of One Harley Quinn) → kGM4uYZzfu0\n",
      "✅ Found: Lying And Stealing → mKXa10VDQj4\n",
      "✅ Found: Thriller → CAt339xT-ZE\n",
      "✅ Found: Bigger → QbTJiAKvww0\n",
      "✅ Found: Sonic The Hedgehog → szby7ZHLnkA\n",
      "✅ Found: Blumhouse's Fantasy Island → a6O30nJ02PU\n",
      "✅ Found: Lighthouse, The (2019) → Hyag7lR8CPA\n",
      "✅ Found: Call of the Wild, The → 5P8R2zAhEwg\n",
      "✅ Found: Gretel And Hansel → QZblQLhKcZQ\n",
      "✅ Found: Trauma Center → Y2DrOhVpPZo\n",
      "✅ Found: Gentlemen, The → Ify9S7hj480\n",
      "✅ Found: Brahms: The Boy II → ytxEldPKnyA\n",
      "✅ Found: Like A Boss → 9ESkyRFEso4\n",
      "✅ Done scraping video IDs for batch 4.\n"
     ]
    }
   ],
   "source": [
    "####### scraping trailer comments for batch 4\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_4_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_4_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed69290d-9524-46c8-9732-afaf0d3962d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_4_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abffefaf-9dc7-4a55-8f5a-4881d9aa19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # use your actual API key again\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7d2517-d20a-49e1-93ec-2b0e194aeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape comments for fetched video IDs\n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58c11880-a2fe-48a8-9ce9-f3901cf0caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Shakira In Concert: El Dorado World Tour\n",
      "Scraping: Ford v. Ferrari\n",
      "Scraping: Motherless Brooklyn\n",
      "Scraping: Frozen 2 (2019)\n",
      "Scraping: Depeche Mode: Spirits In The Forest\n",
      "Scraping: MET Opera: Akhnaten (2019)\n",
      "Scraping: Knives Out\n",
      "Scraping: Charlie's Angels\n",
      "Scraping: Where'd You Go Bernadette\n",
      "Scraping: Last Christmas\n",
      "Scraping: Aeronauts, The\n",
      "Scraping: Jumanji: The Next Level\n",
      "Scraping: 21 Bridges\n",
      "Scraping: Star Wars: The Rise Of Skywalker\n",
      "Scraping: Can You Keep A Secret?\n",
      "Scraping: Apocalypse Now Final Cut (2019)\n",
      "Scraping: Murder Of Nicole Brown Simpson, The\n",
      "Scraping: Spies In Disguise\n",
      "Scraping: Jexi\n",
      "Scraping: Cats\n",
      "Scraping: Grudge, The\n",
      "Scraping: Richard Jewell\n",
      "Scraping: Plus One\n",
      "Scraping: Beyond the mountains and hills\n",
      "Scraping: Dark Waters\n",
      "Scraping: Primal\n",
      "Scraping: Farewell, The\n",
      "Scraping: Bad Boys For Life\n",
      "Scraping: Dolittle\n",
      "Scraping: Just Mercy\n",
      "Scraping: 1917\n",
      "Scraping: Bombshell\n",
      "Scraping: Disturbing The Peace\n",
      "Scraping: Wedding Year, The\n",
      "Scraping: Little Women\n",
      "Scraping: Jojo Rabbit\n",
      "Scraping: Turning, The\n",
      "Scraping: Birds Of Prey (And The Fantabulous Emancipation Of One Harley Quinn)\n",
      "Scraping: Lying And Stealing\n",
      "Scraping: Thriller\n",
      "Scraping: Bigger\n",
      "Scraping: Sonic The Hedgehog\n",
      "Scraping: Blumhouse's Fantasy Island\n",
      "Scraping: Lighthouse, The (2019)\n",
      "Scraping: Call of the Wild, The\n",
      "Scraping: Gretel And Hansel\n",
      "Scraping: Trauma Center\n",
      "Scraping: Gentlemen, The\n",
      "Scraping: Brahms: The Boy II\n",
      "Scraping: Like A Boss\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch4 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch4.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e8895b-d8b0-4816-a12f-bebece01daf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 173814 comments for batch 4.\n"
     ]
    }
   ],
   "source": [
    "comments_df_4 = pd.DataFrame(all_comments_batch4)\n",
    "comments_df_4.to_csv(\"comments_batch_4.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_4)} comments for batch 4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc406cc6-6eb1-49af-a754-68ece39492ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Onward → gn5QmllRCn4\n",
      "✅ Found: Invisible Man, The (2020) → dSBsNeYqh-k\n",
      "✅ Found: I See You (2019) → 1z3bGnYhMis\n",
      "✅ Found: Last Full Measure, The → Go8zI2sytEc\n",
      "✅ Found: Wolf Hour, The → SPtqP0-zG64\n",
      "✅ Found: Bloodshot → vOUVVDWdXbo\n",
      "✅ Found: Way Back, The (2020) → VzNJVSsjE-I\n",
      "✅ Found: Family Romance, LLC → y36LeFtKjSQ\n",
      "✅ Found: Galveston → Hfdxq5eLrxM\n",
      "✅ Found: Vault → L41IJiageyg\n",
      "✅ Found: My Spy → pfAhQSz-j_o\n",
      "✅ Found: Give Me Liberty → cuU4k3P6YCA\n",
      "✅ Found: Hail Satan? → 27RtJp-rhHk\n",
      "✅ Found: Night Clerk, The → 8j6wDoyiyd4\n",
      "✅ Found: Survive The Night → TfTNGqXBwHY\n",
      "✅ Found: Burden (2020) → DSLhLfAnVd8\n",
      "✅ Found: Burnt Orange Heresy, The → 6PMAlfrdQNI\n",
      "✅ Found: Arkansas → 2oDRSS67Ohw\n",
      "✅ Found: Becky → 5-j5HtaOSjA\n",
      "✅ Found: Ophelia → WU1uwl9cniw\n",
      "✅ Found: Capone → 2J5OE16C_zY\n",
      "✅ Found: Think Like A Dog → s3_5WZSOsn8\n",
      "✅ Found: Scoob! → C9U_v_KuEb4\n",
      "✅ Found: Outpost, The (2020) → f4LM9a02q9Q\n",
      "✅ Found: High Note, The (2020) → cy8lUfePOZs\n",
      "✅ Found: Inception (2010) (re) → YoHD9XEInc0\n",
      "✅ Found: Endless → FTzU_zByE3Y\n",
      "✅ Found: Elvis: That's The Way It Is: Special Edition (re) → WXwPJ6ImFaY\n",
      "✅ Found: Secret: Dare To Dream, The → Mf3-oCDdTzQ\n",
      "✅ Found: Force Of Nature (2020) → d61GX5VoEJc\n",
      "✅ Found: I Still Believe → 7Za7-Q8YURM\n",
      "✅ Found: King of Staten Island, The → azkVr0VUSTA\n",
      "✅ Found: Tenet → L3pk_TBkihU\n",
      "✅ Found: New Mutants, The → W_vJhUAOFpI\n",
      "✅ Found: After We Collided → ndyVh4GFMw0\n",
      "✅ Found: Animal Crackers → 6yrtrDjmw5c\n",
      "✅ Found: Unhinged → vdxDXoODKN8\n",
      "✅ Found: Rental, The → MGdcTUMGxB0\n",
      "✅ Found: Greenland → 1AyxdYP1SNc\n",
      "✅ Found: Broken Hearts Gallery, The → 7aokhRzwlJI\n",
      "✅ Found: Antebellum → 7MPib67BDHY\n",
      "✅ Found: Inheritance (2020) → 3q7JOLZxjqY\n",
      "✅ Found: Ava → ozUuAcGOhPs\n",
      "✅ Found: Bill & Ted Face The Music → 0hAL7emClFM\n",
      "✅ Found: Honest Thief → jG1X67vnYM0\n",
      "✅ Found: Trolls World Tour → 3cCoDDhFpmY\n",
      "✅ Found: Come Play → LQwiqhdMQ7g\n",
      "✅ Found: Witches, The → 9nlhmJF5FNI\n",
      "✅ Found: Craft: Legacy, The → J60ueFp-jv8\n",
      "✅ Found: Superintelligence → XN6JawOGvpM\n",
      "✅ Found: Jiu Jitsu → ywhTeWg8970\n",
      "✅ Found: David Byrne's American Utopia → lg4hcgtjDPc\n",
      "✅ Found: Wonder Woman 1984 → sfM7_JLk-84\n",
      "✅ Found: Angels Fallen → u9ioVD15feE\n",
      "✅ Found: Follow Me → IAUeZAO-Klw\n",
      "✅ Found: Love, Weddings & Other Disasters → GkKenPum1ts\n",
      "✅ Found: Fatman → y2IDILXUE-k\n",
      "✅ Found: Nomadland → 6sxCFZ8_d84\n",
      "✅ Found: Raya And The Last Dragon → 1VIZ89FEjYI\n",
      "✅ Found: Mortal Kombat → NYH2sLid0Zc\n",
      "✅ Found: Music (Event) → OnbIZM0KKq0\n",
      "✅ Found: Tom & Jerry → kP9TfCWaQT4\n",
      "✅ Found: Godzilla vs. Kong → odM92ap8_c0\n",
      "✅ Found: Unholy, The → NmQiJPLYzPI\n",
      "✅ Found: Promising Young Woman → 7i5kiFDunk8\n",
      "✅ Found: No Man's Land (2020) → hMMD7J73oS8\n",
      "❌ Error for 'Spiral (dir. Bousman)': list index out of range\n",
      "✅ Found: Monster Hunter → 3od-kQMTZ9M\n",
      "✅ Found: Those Who Wish Me Dead → sV6VNNjBkcE\n",
      "✅ Found: Minari → KQ0gFidlro8\n",
      "✅ Found: Friendsgiving → Hf-efq9o3Nk\n",
      "✅ Found: Peter Rabbit 2: The Runaway → euGHcnyUo84\n",
      "✅ Found: Wrath Of Man → EFYEni2gsK0\n",
      "✅ Found: United States vs. Billie Holiday, The → USi-ppCfxEA\n",
      "✅ Found: Cruella → gmRKv7n2If8\n",
      "✅ Found: Quiet Place Part II, A → BpdDN9d9Jio\n",
      "✅ Found: Nobody → -5X2pt95cIo\n",
      "✅ Found: Blackbird (2020) → I9kb6ZJre78\n",
      "✅ Found: Tesla → e4U-23TOKms\n",
      "✅ Found: Conjuring: The Devil Made Me Do It, The → h9Q4zZS2v1k\n",
      "✅ Found: Spirit Untamed → 9jG1nnQGpdI\n",
      "✅ Found: Vanquish → fUElfp3DJ5Y\n",
      "✅ Found: Synchronic → eLkbgpEQ14U\n",
      "✅ Found: Be Natural: The Untold Story of Alice Guy-Blache → ySkfSavijqo\n",
      "✅ Found: Lusty Men, The → FwgUbokUfko\n",
      "✅ Found: Chaos Walking → nRf4ZgzHoVw\n",
      "✅ Found: In the Heights → U0CL-ZSuCrQ\n",
      "✅ Found: Marksman, The → lEBPNi4bEbc\n",
      "✅ Found: Bon Jovi From Encore Nights → sZTx_iiP750\n",
      "✅ Found: Hustler, The (1961) → KND8JCBtkbY\n",
      "✅ Found: Hitman's Wife's Bodyguard, The → 9C0l31YcahQ\n",
      "✅ Found: Extinct → bk0Q-zuUFfc\n",
      "✅ Found: Friend, The (2019) → RIeGP8lTi-U\n",
      "✅ Found: F9 The Fast Saga → PC5blZ9n810\n",
      "✅ Found: Female Trouble → 1U6660xHVJE\n",
      "✅ Found: Fat City → PrfClhgXwNE\n",
      "✅ Found: Croods: A New Age, The → XUN5EEDwHcI\n",
      "✅ Found: Ice Road, The → -zjikObAJHk\n",
      "✅ Found: First Cow → SRUWVT87mt8\n",
      "✅ Found: Four Good Days → uDDCulgiqs4\n",
      "✅ Done scraping video IDs for batch 5.\n"
     ]
    }
   ],
   "source": [
    "############## batch 5\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_5_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_5_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844bdb0a-697a-4021-a2fc-64ec13440f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_5_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30634ad3-48fd-4b4e-9060-a1a862e12f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # use your actual API key again\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7ffaa3-8d98-4290-9f6b-0c204cb341f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bfe041c-4002-431e-abe1-7530878b4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Onward\n",
      "Scraping: Invisible Man, The (2020)\n",
      "Scraping: I See You (2019)\n",
      "Scraping: Last Full Measure, The\n",
      "Scraping: Wolf Hour, The\n",
      "Scraping: Bloodshot\n",
      "Scraping: Way Back, The (2020)\n",
      "Scraping: Family Romance, LLC\n",
      "Scraping: Galveston\n",
      "Scraping: Vault\n",
      "Scraping: My Spy\n",
      "Scraping: Give Me Liberty\n",
      "Scraping: Hail Satan?\n",
      "Scraping: Night Clerk, The\n",
      "Scraping: Survive The Night\n",
      "Scraping: Burden (2020)\n",
      "Scraping: Burnt Orange Heresy, The\n",
      "Scraping: Arkansas\n",
      "Scraping: Becky\n",
      "Scraping: Ophelia\n",
      "Scraping: Capone\n",
      "Scraping: Think Like A Dog\n",
      "Scraping: Scoob!\n",
      "Scraping: Outpost, The (2020)\n",
      "Scraping: High Note, The (2020)\n",
      "Scraping: Inception (2010) (re)\n",
      "Scraping: Endless\n",
      "Scraping: Elvis: That's The Way It Is: Special Edition (re)\n",
      "Scraping: Secret: Dare To Dream, The\n",
      "Scraping: Force Of Nature (2020)\n",
      "Scraping: I Still Believe\n",
      "Scraping: King of Staten Island, The\n",
      "Scraping: Tenet\n",
      "Scraping: New Mutants, The\n",
      "Scraping: After We Collided\n",
      "Scraping: Animal Crackers\n",
      "Scraping: Unhinged\n",
      "Scraping: Rental, The\n",
      "Scraping: Greenland\n",
      "Scraping: Broken Hearts Gallery, The\n",
      "Scraping: Antebellum\n",
      "Scraping: Inheritance (2020)\n",
      "Scraping: Ava\n",
      "Scraping: Bill & Ted Face The Music\n",
      "Scraping: Honest Thief\n",
      "Scraping: Trolls World Tour\n",
      "Scraping: Come Play\n",
      "Scraping: Witches, The\n",
      "Scraping: Craft: Legacy, The\n",
      "Scraping: Superintelligence\n",
      "Scraping: Jiu Jitsu\n",
      "Scraping: David Byrne's American Utopia\n",
      "Scraping: Wonder Woman 1984\n",
      "Scraping: Angels Fallen\n",
      "Scraping: Follow Me\n",
      "Scraping: Love, Weddings & Other Disasters\n",
      "Scraping: Fatman\n",
      "Scraping: Nomadland\n",
      "Scraping: Raya And The Last Dragon\n",
      "Scraping: Mortal Kombat\n",
      "Scraping: Music (Event)\n",
      "Scraping: Tom & Jerry\n",
      "Scraping: Godzilla vs. Kong\n",
      "Scraping: Unholy, The\n",
      "Scraping: Promising Young Woman\n",
      "Scraping: No Man's Land (2020)\n",
      "Scraping: Monster Hunter\n",
      "Scraping: Those Who Wish Me Dead\n",
      "Scraping: Minari\n",
      "Scraping: Friendsgiving\n",
      "Scraping: Peter Rabbit 2: The Runaway\n",
      "Scraping: Wrath Of Man\n",
      "Scraping: United States vs. Billie Holiday, The\n",
      "Scraping: Cruella\n",
      "Scraping: Quiet Place Part II, A\n",
      "Scraping: Nobody\n",
      "Scraping: Blackbird (2020)\n",
      "Scraping: Tesla\n",
      "Scraping: Conjuring: The Devil Made Me Do It, The\n",
      "Scraping: Spirit Untamed\n",
      "Scraping: Vanquish\n",
      "Scraping: Synchronic\n",
      "Scraping: Be Natural: The Untold Story of Alice Guy-Blache\n",
      "Scraping: Lusty Men, The\n",
      "Scraping: Chaos Walking\n",
      "Scraping: In the Heights\n",
      "Scraping: Marksman, The\n",
      "Scraping: Bon Jovi From Encore Nights\n",
      "Scraping: Hustler, The (1961)\n",
      "Scraping: Hitman's Wife's Bodyguard, The\n",
      "Scraping: Extinct\n",
      "Scraping: Friend, The (2019)\n",
      "Scraping: F9 The Fast Saga\n",
      "Scraping: Female Trouble\n",
      "Scraping: Fat City\n",
      "Scraping: Croods: A New Age, The\n",
      "Scraping: Ice Road, The\n",
      "Scraping: First Cow\n",
      "Scraping: Four Good Days\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch5 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch5.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1958ab9-0ca9-4841-ae4f-5a4b94dcd8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 200032 comments for batch 5.\n"
     ]
    }
   ],
   "source": [
    "comments_df_5 = pd.DataFrame(all_comments_batch5)\n",
    "comments_df_5.to_csv(\"comments_batch_5.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_5)} comments for batch 5.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f27d90-6f2b-4e9e-a8c3-14a17abce757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Stand-In, The → b0ZKeBUUtr0\n",
      "✅ Found: Black Widow → M3khLMdKJIU\n",
      "✅ Found: Every Breath You Take → 9dMHedhCYOI\n",
      "✅ Found: Escape Room: Tournament of Champions → KlfUbZJVInA\n",
      "✅ Found: Space Jam: A New Legacy → olXYZOsXw_o\n",
      "✅ Found: Gunpowder Milkshake → H7ZrjXuNgmA\n",
      "✅ Found: Snake Eyes → Vd2sm63Xwfw\n",
      "✅ Found: Comeback Trail, The → H9aJATnSseo\n",
      "✅ Found: Sweet Thing → YKUBI8mrt2o\n",
      "✅ Found: Jungle Cruise → f_HvoipFcA8\n",
      "✅ Found: Old → A4U2pMRV9_k\n",
      "✅ Found: Snowpiercer (2013) (re) → -hsPIBFOHOI\n",
      "✅ Found: Suicide Squad, The → JuDLepNa7hw\n",
      "✅ Found: To the Stars (dir. Stephens) → T5OhkFY1PQE\n",
      "✅ Found: Above Suspicion → 5hta-4X7Z50\n",
      "✅ Found: Night Of The Living Dead (1968) → pd5a0gs-UCQ\n",
      "✅ Found: Respect → qTtxoz3OIlU\n",
      "✅ Found: Don't Breathe 2 → gRbG2tjHYCA\n",
      "✅ Found: Boss Level → PGHD0bqjZWQ\n",
      "✅ Found: Echo Boomers → 8CJivh4gr-Q\n",
      "✅ Found: Little Shop Of Horrors, The (1960) → QqFZuR6UzjA\n",
      "✅ Found: Free Guy → X2m-08cOAbc\n",
      "✅ Found: Paw Patrol → LRMTr2VZcr8\n",
      "✅ Found: Reminiscence → _BggT--yxf0\n",
      "✅ Found: Barb And Star Go To Vista del Mar → yvmDgXfYPbA\n",
      "✅ Found: Carnival Of Souls → tqMEbW7Pe2I\n",
      "✅ Found: Boss Baby: Family Business, The → prWBrMJOg2k\n",
      "✅ Found: Candyman → tlwzuZ9kOQU\n",
      "✅ Found: Protege, The → fSqa0a3mGk8\n",
      "✅ Found: Queenpins → v90YxH6lR_8\n",
      "✅ Found: Shang-Chi and the Legend of the Ten Rings → 8YjFbMbfXaQ\n",
      "✅ Found: After We Fell (Event) → SLdPPObqlmc\n",
      "✅ Found: Come Away → 9PpfMd5x3pk\n",
      "✅ Found: Malignant → Gczt0fhawDs\n",
      "✅ Found: Rock Dog 2 → UoYB1Zn6lR4\n",
      "✅ Found: Green Knight, The → sS6ksY8xWCY\n",
      "✅ Found: What Is Life Worth → 1dENT2CRCKo\n",
      "✅ Found: Cry Macho → JVc8SI5CAKw\n",
      "✅ Found: Midnight in the Switchgrass → 1pNN66fRumw\n",
      "✅ Found: Dreamland (2020) → agn4t4SOB_0\n",
      "✅ Found: Till Death → Wfmb1-UEpW4\n",
      "✅ Found: Copshop → ezHiQwwzEjU\n",
      "✅ Found: No Time To Die → BIhNsAtPbPI\n",
      "✅ Found: Songbird → IgxXSfto6Vo\n",
      "✅ Found: Fatima → d1f_9_TGlpI\n",
      "✅ Found: Misfits, The → lxhMLjt83KQ\n",
      "✅ Found: Run → 0Dhh7q9Us5c\n",
      "✅ Found: Flag Day → 2D-3fxTLmS8\n",
      "✅ Found: Seder-Masochism → zTlkxGPRiPc\n",
      "✅ Found: Venom: Let There Be Carnage → -FmWuCgJmxo\n",
      "✅ Found: Ron's Gone Wrong → fCqGfjBSk0I\n",
      "✅ Found: Ferdinand (2017) (re) → oVQdZ2Ac6Hw\n",
      "✅ Found: Sita Sings The Blues → 8pLtEbffdAM\n",
      "✅ Found: Dune → n9xhJrPXop4\n",
      "✅ Found: Halloween Kills → hL6R3HmQfPc\n",
      "✅ Found: Ailey → PHcM4HJEgs4\n",
      "✅ Found: Addams Family 2, The → Kd82bSBDE84\n",
      "✅ Found: Last Duel, The → mgygUwPJvYk\n",
      "✅ Found: Last Night In Soho → AcVnFrxjPjI\n",
      "✅ Found: Mercy Black → BQHcRb1DMsQ\n",
      "✅ Found: Eternals → G54feZ72tuk\n",
      "✅ Found: Doors: Live at the Bowl '68 Special Edition, The → kWgXtsJzgZI\n",
      "✅ Found: Red Notice → Pj0wz7zu3Ms\n",
      "✅ Found: French Dispatch, The → TcPk2p0Zaw4\n",
      "✅ Found: Antlers → 2aiYxwVuZ1o\n",
      "✅ Found: Doorman, The → pugLE1ifvbY\n",
      "✅ Found: King Richard → BKP_0z52ZAw\n",
      "✅ Found: Card Counter, The → 7RvVT1cDiNc\n",
      "✅ Found: Encanto → CaimKeDcudo\n",
      "✅ Found: House Of Gucci → pGi3Bgn7U5U\n",
      "✅ Found: Ghostbusters: Afterlife → ahZFCF--uRY\n",
      "✅ Found: Sing 2 → EPZu5MA2uqI\n",
      "✅ Found: West Side Story → A5GJLwWiYSg\n",
      "✅ Found: Pig → 1i-_CRKdh4Y\n",
      "✅ Found: Don't Look Up → RbIxYm3mKzI\n",
      "✅ Found: Kid, The (Dir. Chaplin) → 89iqMJ2iiOI\n",
      "✅ Found: TCM: West Side Story 60th Anniversary → 7CfPT9N9ZM0\n",
      "✅ Found: Spider-Man: No Way Home → JfVOs4VSpmA\n",
      "✅ Found: Clifford The Big Red Dog → 4zH5iYM4wJo\n",
      "✅ Found: Matrix Resurrections, The → 9ix7TUGVYIo\n",
      "✅ Found: King's Man, The → 5zdBG-iGfes\n",
      "✅ Found: Double Play: James Benning and Richard Linklater → eU9jE0RVWI0\n",
      "✅ Found: Hating Game, The → j3qBGOD4b4A\n",
      "✅ Found: Out of Death → UlA_achIbk4\n",
      "✅ Found: 13 Minutes → YiEFZLRyRQo\n",
      "✅ Found: Punch-Drunk Love (2002) (re) → QcGzJ6ppIso\n",
      "✅ Found: Scream → beToTslH17s\n",
      "✅ Found: Arctic Dogs → KppQoNsD5qY\n",
      "✅ Found: Penguin Bloom → q7eZEZHRrVg\n",
      "✅ Found: Nightmare Alley → Q81Yf46Oj3s\n",
      "✅ Found: 355, The → SV0s2S9reT0\n",
      "✅ Found: Last Picture Show, The → Z1-GbAL5dAI\n",
      "✅ Found: Swallow → CUpMcNIY5ms\n",
      "✅ Found: Paper Moon → tYBs63-mArU\n",
      "✅ Found: What's Up, Doc? → UhK8O6It_3U\n",
      "✅ Found: Torn → w916HoOtSoE\n",
      "✅ Found: Stranger, The (1946) → zubpk4o5lLQ\n",
      "✅ Found: Moonfall → ivIwdQBlS10\n",
      "✅ Found: Lost Daughter, The (2021) → xNq9YOfL0Zs\n",
      "✅ Found: Boy Behind the Door, The → 3HEmC-ASveA\n",
      "✅ Done scraping video IDs for batch 6.\n"
     ]
    }
   ],
   "source": [
    "####### batch 6\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_6_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_6_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3bd9114-9aa7-489e-b584-821028386228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_6_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a130856-224c-4a09-b8a3-7a480a53cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d85f13a-655b-4b60-ad70-4690b2903499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ecd7d06-8e90-4207-88d1-40881e54e7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Stand-In, The\n",
      "Scraping: Black Widow\n",
      "Scraping: Every Breath You Take\n",
      "Scraping: Escape Room: Tournament of Champions\n",
      "Scraping: Space Jam: A New Legacy\n",
      "Scraping: Gunpowder Milkshake\n",
      "Scraping: Snake Eyes\n",
      "Scraping: Comeback Trail, The\n",
      "Scraping: Sweet Thing\n",
      "Scraping: Jungle Cruise\n",
      "Scraping: Old\n",
      "Scraping: Snowpiercer (2013) (re)\n",
      "Scraping: Suicide Squad, The\n",
      "Scraping: To the Stars (dir. Stephens)\n",
      "Scraping: Above Suspicion\n",
      "Scraping: Night Of The Living Dead (1968)\n",
      "Scraping: Respect\n",
      "Scraping: Don't Breathe 2\n",
      "Scraping: Boss Level\n",
      "Scraping: Echo Boomers\n",
      "Scraping: Little Shop Of Horrors, The (1960)\n",
      "Scraping: Free Guy\n",
      "Scraping: Paw Patrol\n",
      "Scraping: Reminiscence\n",
      "Scraping: Barb And Star Go To Vista del Mar\n",
      "Scraping: Carnival Of Souls\n",
      "Scraping: Boss Baby: Family Business, The\n",
      "Scraping: Candyman\n",
      "Scraping: Protege, The\n",
      "Scraping: Queenpins\n",
      "Scraping: Shang-Chi and the Legend of the Ten Rings\n",
      "Scraping: After We Fell (Event)\n",
      "Scraping: Come Away\n",
      "Scraping: Malignant\n",
      "Scraping: Rock Dog 2\n",
      "Scraping: Green Knight, The\n",
      "Scraping: What Is Life Worth\n",
      "Scraping: Cry Macho\n",
      "Scraping: Midnight in the Switchgrass\n",
      "Scraping: Dreamland (2020)\n",
      "Scraping: Till Death\n",
      "Scraping: Copshop\n",
      "Scraping: No Time To Die\n",
      "Scraping: Songbird\n",
      "Scraping: Fatima\n",
      "Scraping: Misfits, The\n",
      "Scraping: Run\n",
      "Scraping: Flag Day\n",
      "Scraping: Seder-Masochism\n",
      "Scraping: Venom: Let There Be Carnage\n",
      "Scraping: Ron's Gone Wrong\n",
      "Scraping: Ferdinand (2017) (re)\n",
      "Scraping: Sita Sings The Blues\n",
      "Scraping: Dune\n",
      "Scraping: Halloween Kills\n",
      "Scraping: Ailey\n",
      "Scraping: Addams Family 2, The\n",
      "Scraping: Last Duel, The\n",
      "Scraping: Last Night In Soho\n",
      "Scraping: Mercy Black\n",
      "Scraping: Eternals\n",
      "Scraping: Doors: Live at the Bowl '68 Special Edition, The\n",
      "Scraping: Red Notice\n",
      "Scraping: French Dispatch, The\n",
      "Scraping: Antlers\n",
      "Scraping: Doorman, The\n",
      "Scraping: King Richard\n",
      "Scraping: Card Counter, The\n",
      "Scraping: Encanto\n",
      "Scraping: House Of Gucci\n",
      "Scraping: Ghostbusters: Afterlife\n",
      "Scraping: Sing 2\n",
      "Scraping: West Side Story\n",
      "Scraping: Pig\n",
      "Scraping: Don't Look Up\n",
      "Scraping: Kid, The (Dir. Chaplin)\n",
      "Scraping: TCM: West Side Story 60th Anniversary\n",
      "Scraping: Spider-Man: No Way Home\n",
      "Scraping: Clifford The Big Red Dog\n",
      "Scraping: Matrix Resurrections, The\n",
      "Scraping: King's Man, The\n",
      "Scraping: Double Play: James Benning and Richard Linklater\n",
      "Scraping: Hating Game, The\n",
      "Scraping: Out of Death\n",
      "Scraping: 13 Minutes\n",
      "Scraping: Punch-Drunk Love (2002) (re)\n",
      "Scraping: Scream\n",
      "Scraping: Arctic Dogs\n",
      "Scraping: Penguin Bloom\n",
      "Scraping: Nightmare Alley\n",
      "Scraping: 355, The\n",
      "Scraping: Last Picture Show, The\n",
      "Scraping: Swallow\n",
      "Scraping: Paper Moon\n",
      "Scraping: What's Up, Doc?\n",
      "Scraping: Torn\n",
      "Scraping: Stranger, The (1946)\n",
      "Scraping: Moonfall\n",
      "Scraping: Lost Daughter, The (2021)\n",
      "Scraping: Boy Behind the Door, The\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch6 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch6.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "241801ab-79c3-47cc-baa8-1bb57b99ce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 368716 comments for batch 6.\n"
     ]
    }
   ],
   "source": [
    "comments_df_6 = pd.DataFrame(all_comments_batch6)\n",
    "comments_df_6.to_csv(\"comments_batch_6.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_6)} comments for batch 6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fc8be7-142f-4592-8a4e-a1b1f28c1f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Twin Peaks: The Missing Pieces → bKoAf86-g2Q\n",
      "✅ Found: Death on the Nile → dZRqB0JLizw\n",
      "✅ Found: Marry Me → Ebv9_rNb5Ig\n",
      "✅ Found: Beatles: Get Back - The Rooftop Concert, The → Auta2lagtw4\n",
      "✅ Found: Uncharted → eHp3MbsCbMg\n",
      "✅ Found: Prisoners Of The Ghostland → CeIfnDc0jvA\n",
      "✅ Found: Blackboard Jungle (re: 1996) → Vb8BYNsAYHc\n",
      "✅ Found: Band of Angels (re: 2009) → M8HswczNQoc\n",
      "✅ Found: Cyrano → 5e8apSFDXsQ\n",
      "✅ Found: Quiet Man, The → 2kimSAyuIYc\n",
      "✅ Found: Batman, The → mqqft2x_Aa4\n",
      "✅ Found: Jackass Forever → FNq-QT2Jpng\n",
      "✅ Found: Koati → 48yg4TFVIdI\n",
      "✅ Found: Dog → V4tAtp-TyzQ\n",
      "✅ Found: Desperate Hour, The → kiwCH84GMhE\n",
      "✅ Found: Ambulance → 7NU-STboFeI\n",
      "✅ Found: X (2022) → Awg3cWuHfoc\n",
      "✅ Found: Contractor, The → e7glvM8Xh0w\n",
      "✅ Found: Georgetown → -ch3BdXOgyc\n",
      "✅ Found: Sonic The Hedgehog 2 → G5kzUpWAusI\n",
      "✅ Found: Morbius → oZ6iiRrz1SY\n",
      "✅ Found: Lighthouse, The (2019) (re) → SrlAsjra0qc\n",
      "✅ Found: Fantastic Beasts: The Secrets of Dumbledore → Y9dr2zw-TXQ\n",
      "✅ Found: Everything Everywhere All at Once → wxN1T1uxQ2g\n",
      "✅ Found: Gasoline Alley → 2H4Lfwc0U94\n",
      "✅ Found: Mass → oU56Ns1nXsE\n",
      "✅ Found: Lost City, The → nfKO9rYDmE8\n",
      "✅ Found: Bad Guys, The → HvLHYox_Vq8\n",
      "✅ Found: Demonic → iIIiC4v7ON8\n",
      "✅ Found: Two Rode Together → UhJfAo5RMYI\n",
      "✅ Found: Moby Dick (Re 2011) → fM-7qTW6OjI\n",
      "✅ Found: Tall Men, The (re) → xcMmLTJmO5c\n",
      "✅ Found: Northman, The → oMSdFM12hOw\n",
      "✅ Found: Unbearable Weight Of Massive Talent, The → CKTRbKch2K4\n",
      "✅ Found: Cheyenne Autumn → 1rn77EmA5ek\n",
      "✅ Found: Bend of the River (re) → kkJU9H6ol5k\n",
      "✅ Found: Dangerous (2021) → rFq52e7wYws\n",
      "✅ Found: Seventh Day, The → HJThAgic45Q\n",
      "✅ Found: Fortress (2021) → GNGXauix9Mo\n",
      "✅ Found: Unconquered (re) → sDVGOZpvsxI\n",
      "✅ Found: Doctor Strange in the Multiverse of Madness → USBMj93ODog\n",
      "✅ Found: Reel Rock 16 → HtM4-K_CCZA\n",
      "✅ Found: Last Looks → KvdfPq6HIzI\n",
      "✅ Found: After Yang → Kwp32zLc08c\n",
      "✅ Found: Ox-Bow Incident, The (re 05) → 9FM-mpfEk6M\n",
      "✅ Found: Memory (2022) → yGw8yw6Mso8\n",
      "✅ Found: Twenty One Pilots Cinema Experience → 23gp-Sf1Oo8\n",
      "✅ Found: All That Jazz → SMqyZKNAVuU\n",
      "✅ Found: Wild at Heart → n2YCseaZK0Q\n",
      "✅ Found: Barton Fink → 7Hj2vzqNgKU\n",
      "✅ Done scraping video IDs for batch 7.\n"
     ]
    }
   ],
   "source": [
    "########### batch 7\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_7_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_7_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9536e56c-2728-4649-a455-4e5a4c07ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_7_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "592adaa1-86c1-4565-83e5-afe92695e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fefe55e-0d09-4570-80c8-10beee2f3902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "608ba095-6ce1-4649-a018-f6e7f33d8416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Twin Peaks: The Missing Pieces\n",
      "Scraping: Death on the Nile\n",
      "Scraping: Marry Me\n",
      "Scraping: Beatles: Get Back - The Rooftop Concert, The\n",
      "Scraping: Uncharted\n",
      "Scraping: Prisoners Of The Ghostland\n",
      "Scraping: Blackboard Jungle (re: 1996)\n",
      "Scraping: Band of Angels (re: 2009)\n",
      "Scraping: Cyrano\n",
      "Scraping: Quiet Man, The\n",
      "Scraping: Batman, The\n",
      "Scraping: Jackass Forever\n",
      "Scraping: Koati\n",
      "Scraping: Dog\n",
      "Scraping: Desperate Hour, The\n",
      "Scraping: Ambulance\n",
      "Scraping: X (2022)\n",
      "Scraping: Contractor, The\n",
      "Scraping: Georgetown\n",
      "Scraping: Sonic The Hedgehog 2\n",
      "Scraping: Morbius\n",
      "Scraping: Lighthouse, The (2019) (re)\n",
      "Scraping: Fantastic Beasts: The Secrets of Dumbledore\n",
      "Scraping: Everything Everywhere All at Once\n",
      "Scraping: Gasoline Alley\n",
      "Scraping: Mass\n",
      "Scraping: Lost City, The\n",
      "Scraping: Bad Guys, The\n",
      "Scraping: Demonic\n",
      "Scraping: Two Rode Together\n",
      "Scraping: Moby Dick (Re 2011)\n",
      "Scraping: Tall Men, The (re)\n",
      "Scraping: Northman, The\n",
      "Scraping: Unbearable Weight Of Massive Talent, The\n",
      "Scraping: Cheyenne Autumn\n",
      "Scraping: Bend of the River (re)\n",
      "Scraping: Dangerous (2021)\n",
      "Scraping: Seventh Day, The\n",
      "Scraping: Fortress (2021)\n",
      "Scraping: Unconquered (re)\n",
      "Scraping: Doctor Strange in the Multiverse of Madness\n",
      "Scraping: Reel Rock 16\n",
      "Scraping: Last Looks\n",
      "Scraping: After Yang\n",
      "Scraping: Ox-Bow Incident, The (re 05)\n",
      "Scraping: Memory (2022)\n",
      "Scraping: Twenty One Pilots Cinema Experience\n",
      "Scraping: All That Jazz\n",
      "Scraping: Wild at Heart\n",
      "Scraping: Barton Fink\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch7 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch7.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "909e9d74-186c-4fb4-bb2f-82972a277e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 103705 comments for batch 7.\n"
     ]
    }
   ],
   "source": [
    "comments_df_7 = pd.DataFrame(all_comments_batch7)\n",
    "comments_df_7.to_csv(\"comments_batch_7.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_7)} comments for batch 7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55f2901-e6c4-4c7b-a421-4412fb672628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Sex, Lies and Videotape → uajUDN4AiEM\n",
      "✅ Found: Conversation, The (1974) (re) → Q8TUSsqWCTM\n",
      "✅ Found: Top Gun Maverick → qSqVVswa420\n",
      "✅ Found: Marmaduke → wDXYjxxnALU\n",
      "✅ Found: Gypsy Moon → BkhqVckCqow\n",
      "✅ Found: Caja, La → HyPekwtu8OM\n",
      "✅ Found: Jurassic World Dominion → fb5ELWi-ekk\n",
      "✅ Found: Lightyear → 9JyNLR3Xx5U\n",
      "✅ Found: Elvis → wBDLRvjHVOY\n",
      "✅ Found: Black Phone, The → 3eGP6im8AZA\n",
      "✅ Found: Deadlock (2021) → UoqRfsoa5c4\n",
      "✅ Found: Minions: The Rise Of Gru → CM-17oLBJ6U\n",
      "✅ Found: Last Seen Alive → 5Gutb4hc5r4\n",
      "✅ Found: Tokyo-ga → 8R6lVzHVduA\n",
      "✅ Found: Thor: Love And Thunder → zXLCPzB3kZo\n",
      "✅ Found: Easy Rider (re) → RPf2D-N7sSs\n",
      "✅ Found: Gray Man, The → BmllggGO4pM\n",
      "✅ Found: Black Site → Gs9Z-XP57Hk\n",
      "✅ Found: Woman under the Influence, A → iveJ7jFJHpI\n",
      "✅ Found: Opening Night (1977) (re) → EPPUltC2lys\n",
      "✅ Found: Killing Of A Chinese Bookie, The → FW1tVczcuNI\n",
      "✅ Found: Faces → bv5zBu_-T5M\n",
      "✅ Found: Shadows (1959) → jmPU97LOnEI\n",
      "✅ Found: DC League of Super Pets → 1jkw2JPCl18\n",
      "✅ Found: American Siege → AdI6ea_0T0s\n",
      "✅ Found: Book of Love → M_HTbI00qOU\n",
      "✅ Found: Husbands (Re: 2012) → A4U2pMRV9_k\n",
      "✅ Found: Gloria (re: 2010) → ahQ0jSZcOQA\n",
      "✅ Found: Minnie And Moskowitz → ADoVQQhRULc\n",
      "✅ Found: Bullet Train → 0IOsk2Vlc4o\n",
      "✅ Found: Men → pt81CJcWZy8\n",
      "✅ Found: Easter Sunday → YIixb42aJPg\n",
      "✅ Found: Witch, The (2016) → iQXmlf3Sefg\n",
      "✅ Found: Vigil, The → YsMcckth7oU\n",
      "✅ Found: Fortress: Sniper's Eye → uF4f_nlJpvY\n",
      "✅ Found: Nope → In8fuzj3gck\n",
      "✅ Found: Blacklight → PE04ESdgnHI\n",
      "✅ Found: My Fair Lady (1964) → ZJBM6qs22sE\n",
      "✅ Found: TCM: An American In Paris → liP7mRd8RWM\n",
      "✅ Found: Singin' In The Rain (1952) → D-NJHUasYVA\n",
      "✅ Found: New York, New York (1977) → 0uowV7Qf1yU\n",
      "✅ Found: One From The Heart (1982) → Q1uxAjXh2xo\n",
      "✅ Found: South Pacific (1958) → ZIxADKeLI14\n",
      "✅ Found: After Ever Happy (Event) → hLQ-5exgctI\n",
      "✅ Found: Paradise Highway → zjv83li96BE\n",
      "✅ Found: Beast → oQMc7Sq36mI\n",
      "✅ Found: Where The Crawdads Sing → PY3808Iq0Tg\n",
      "✅ Found: E.T. The Extra-Terrestrial (1982) (re: 2022) → txhe4sat07Q\n",
      "✅ Found: Jaws (1975) → wlZYnOLupYY\n",
      "✅ Found: Vanishing Point (1971) → xYKsrEWIapc\n",
      "✅ Done scraping video IDs for batch 8.\n"
     ]
    }
   ],
   "source": [
    "########## batch 8\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_8_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_8_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 8.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3bf337e-7cac-48ef-9969-67c12841508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_8_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c0346f-af26-4c0e-93bb-27a9a79adebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5495a3e3-772b-4a5a-8734-38818102f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22960661-1e95-4139-80f1-9cfe5b7ba15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Sex, Lies and Videotape\n",
      "Scraping: Conversation, The (1974) (re)\n",
      "Scraping: Top Gun Maverick\n",
      "Scraping: Marmaduke\n",
      "Scraping: Gypsy Moon\n",
      "Scraping: Caja, La\n",
      "Scraping: Jurassic World Dominion\n",
      "Scraping: Lightyear\n",
      "Scraping: Elvis\n",
      "Scraping: Black Phone, The\n",
      "Scraping: Deadlock (2021)\n",
      "Scraping: Minions: The Rise Of Gru\n",
      "Scraping: Last Seen Alive\n",
      "Scraping: Tokyo-ga\n",
      "Scraping: Thor: Love And Thunder\n",
      "Scraping: Easy Rider (re)\n",
      "Scraping: Gray Man, The\n",
      "Scraping: Black Site\n",
      "Scraping: Woman under the Influence, A\n",
      "Scraping: Opening Night (1977) (re)\n",
      "Scraping: Killing Of A Chinese Bookie, The\n",
      "Scraping: Faces\n",
      "Scraping: Shadows (1959)\n",
      "Scraping: DC League of Super Pets\n",
      "Scraping: American Siege\n",
      "Scraping: Book of Love\n",
      "Scraping: Husbands (Re: 2012)\n",
      "Scraping: Gloria (re: 2010)\n",
      "Scraping: Minnie And Moskowitz\n",
      "Scraping: Bullet Train\n",
      "Scraping: Men\n",
      "Scraping: Easter Sunday\n",
      "Scraping: Witch, The (2016)\n",
      "Scraping: Vigil, The\n",
      "Scraping: Fortress: Sniper's Eye\n",
      "Scraping: Nope\n",
      "Scraping: Blacklight\n",
      "Scraping: My Fair Lady (1964)\n",
      "Scraping: TCM: An American In Paris\n",
      "Scraping: Singin' In The Rain (1952)\n",
      "Scraping: New York, New York (1977)\n",
      "Scraping: One From The Heart (1982)\n",
      "Scraping: South Pacific (1958)\n",
      "Scraping: After Ever Happy (Event)\n",
      "Scraping: Paradise Highway\n",
      "Scraping: Beast\n",
      "Scraping: Where The Crawdads Sing\n",
      "Scraping: E.T. The Extra-Terrestrial (1982) (re: 2022)\n",
      "Scraping: Jaws (1975)\n",
      "Scraping: Vanishing Point (1971)\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch8 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch8.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82789147-5594-4a26-86fd-e786e6f16728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 111617 comments for batch 8.\n"
     ]
    }
   ],
   "source": [
    "comments_df_8 = pd.DataFrame(all_comments_batch8)\n",
    "comments_df_8.to_csv(\"comments_batch_8.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_8)} comments for batch 8.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573b4de6-893a-4406-b356-20c847765a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Ticket To Paradise → hkP4tVTdsz8\n",
      "✅ Found: Invitation, The → 5bL1ftuxgOE\n",
      "✅ Found: Minamata → WP3pKTssw_E\n",
      "✅ Found: Moonage Daydream → GSbbDCV45OI\n",
      "✅ Found: Panama → Gbdm7vcPdwI\n",
      "✅ Found: Birds, The (Re 2012) → A4U2pMRV9_k\n",
      "✅ Found: Effect of gamma rays on Man-in-the-Moon Marigolds, The (re 2017) → U8lG53h2_LI\n",
      "✅ Found: Rio Bravo (1959) → 1DZRFdYhr4s\n",
      "✅ Found: Wind Across the Everglades → r5eexpL9tWA\n",
      "✅ Found: Avatar (2009) (re) → 5PSNL1qE6VY\n",
      "✅ Found: Don't Worry Darling → SZ2elR3OmWI\n",
      "✅ Found: Smile (2022) → BcDK7lkzzsU\n",
      "✅ Found: 12 Angry Men → TEN-2uTi2c0\n",
      "✅ Found: Anatomy Of A Murder → wVWflGXIj9E\n",
      "✅ Found: Moonfleet → BZ2W5DE-jE8\n",
      "✅ Found: Woman King, The → 3RDaPV_rJ1Y\n",
      "✅ Found: Orphan: First Kill → _uX6of3vBu0\n",
      "✅ Found: Tarnished Angels, The → G73wu6R-Qbw\n",
      "✅ Found: Written on the Wind → PxAPUOjKj_c\n",
      "✅ Found: Rancho Notorious (re:2011) → aark3zMGGVY\n",
      "✅ Found: Halloween Ends → i_mAWKyfj6c\n",
      "✅ Found: Good House, The → xHMH3phhsOk\n",
      "✅ Found: Black Adam → X0tOpBuYasI\n",
      "✅ Found: Cinderella and the Spellbinder → qciXWBU83vM\n",
      "✅ Found: Exorcismo de Dios, El → JAMdGGDxkIQ\n",
      "✅ Found: Lyle, Lyle, Crocodile → s0W6O7mSlaU\n",
      "✅ Found: Amsterdam → GLs2xxM0e78\n",
      "✅ Found: Prey for the Devil → u_jJiZ2oZgs\n",
      "✅ Found: Annie Hall → jQkZXtllZ3k\n",
      "✅ Found: Enforcer, The → Azn3pCtpCC4\n",
      "✅ Found: Fall → iSspRSGc4Dk\n",
      "✅ Found: Confess, Fletch → pb2Pu5EjC1s\n",
      "✅ Found: Duran Duran - A Hollywood High → 9wks26xEaoc\n",
      "✅ Found: Survive the Game → xT8uydi9bDQ\n",
      "✅ Found: Bell Book and Candle → TE7mcmngMFY\n",
      "✅ Found: Searchers, The (re: 2015) → oRH7oaBQuaY\n",
      "✅ Found: Black Panther: Wakanda Forever → X109HCjxVIo\n",
      "✅ Found: She Said → i5pxUQecM3Y\n",
      "✅ Found: Cinderella And The Secret Prince → AGD1DeE7R30\n",
      "✅ Found: Strange World → bKh2G73gCCs\n",
      "✅ Found: Devotion → NCDEGP6VjYY\n",
      "✅ Found: Bros → BQIeBB9XMe8\n",
      "✅ Found: What Ever Happened To Baby Jane? (re: 2004) → 2b9LTSuD-n0\n",
      "✅ Found: Guillermo del Toro's Pinocchio → Od2NW1sfRdA\n",
      "✅ Found: Menu, The → C_uTkUGcHv4\n",
      "✅ Found: Violent Night → a53e4HHnx_s\n",
      "✅ Found: Poker Face → JAZV5DxV3Gw\n",
      "✅ Found: Family Jewels, The → snPppDd6PQM\n",
      "✅ Found: Puss In Boots: The Last Wish → RqrXhwS33yc\n",
      "✅ Found: White Noise → SgwKZAMx_gM\n",
      "✅ Done scraping video IDs for batch 9.\n"
     ]
    }
   ],
   "source": [
    "######### batch 9\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_9_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_9_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 9.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19127a43-9b60-4ca7-8e70-00146d975683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_9_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac33471-f33f-42b8-8a5c-2ac43ecbc3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "681c8ece-0fa1-49a6-9a02-ca78b180e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d243930-57b0-4cae-9679-3ad491e6a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Ticket To Paradise\n",
      "Scraping: Invitation, The\n",
      "Scraping: Minamata\n",
      "Scraping: Moonage Daydream\n",
      "Scraping: Panama\n",
      "Scraping: Birds, The (Re 2012)\n",
      "Scraping: Effect of gamma rays on Man-in-the-Moon Marigolds, The (re 2017)\n",
      "Scraping: Rio Bravo (1959)\n",
      "Scraping: Wind Across the Everglades\n",
      "Scraping: Avatar (2009) (re)\n",
      "Scraping: Don't Worry Darling\n",
      "Scraping: Smile (2022)\n",
      "Scraping: 12 Angry Men\n",
      "Scraping: Anatomy Of A Murder\n",
      "Scraping: Moonfleet\n",
      "Scraping: Woman King, The\n",
      "Scraping: Orphan: First Kill\n",
      "Scraping: Tarnished Angels, The\n",
      "Scraping: Written on the Wind\n",
      "Scraping: Rancho Notorious (re:2011)\n",
      "Scraping: Halloween Ends\n",
      "Scraping: Good House, The\n",
      "Scraping: Black Adam\n",
      "Scraping: Cinderella and the Spellbinder\n",
      "Scraping: Exorcismo de Dios, El\n",
      "Scraping: Lyle, Lyle, Crocodile\n",
      "Scraping: Amsterdam\n",
      "Scraping: Prey for the Devil\n",
      "Scraping: Annie Hall\n",
      "Scraping: Enforcer, The\n",
      "Scraping: Fall\n",
      "Scraping: Confess, Fletch\n",
      "Scraping: Duran Duran - A Hollywood High\n",
      "Scraping: Survive the Game\n",
      "Scraping: Bell Book and Candle\n",
      "Scraping: Searchers, The (re: 2015)\n",
      "Scraping: Black Panther: Wakanda Forever\n",
      "Scraping: She Said\n",
      "Scraping: Cinderella And The Secret Prince\n",
      "Scraping: Strange World\n",
      "Scraping: Devotion\n",
      "Scraping: Bros\n",
      "Scraping: What Ever Happened To Baby Jane? (re: 2004)\n",
      "Scraping: Guillermo del Toro's Pinocchio\n",
      "Scraping: Menu, The\n",
      "Scraping: Violent Night\n",
      "Scraping: Poker Face\n",
      "Scraping: Family Jewels, The\n",
      "Scraping: Puss In Boots: The Last Wish\n",
      "Scraping: White Noise\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch9 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch9.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceaf8464-7f89-4703-9a43-f8f6fdc0a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 64622 comments for batch 9.\n"
     ]
    }
   ],
   "source": [
    "comments_df_9 = pd.DataFrame(all_comments_batch9)\n",
    "comments_df_9.to_csv(\"comments_batch_9.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_9)} comments for batch 9.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19e04d1-ca95-4abf-90c1-83ac34ff4e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Avatar: The Way of Water → d9MyW72ELq0\n",
      "✅ Found: Fabelmans, The → D1G2iLSzOe8\n",
      "✅ Found: Whitney Houston: I Wanna Dance with Somebody → 9tfemzaMkoU\n",
      "✅ Found: Rock Dog 3 Battle the Beat → M_1MLmo_PbA\n",
      "✅ Found: North By Northwest (1959) → DTH7P7Bw4kg\n",
      "✅ Found: Fortune Cookie, The → F65xGJdwTgs\n",
      "✅ Found: Angel (1937) → p0cCFfUiaEM\n",
      "✅ Found: Shotgun Wedding → qNVwRHQL8jw\n",
      "✅ Found: Bed Rest → P1wZvRRJSvs\n",
      "✅ Found: Wrong Place → 8vWueKwt4VM\n",
      "✅ Found: Bitter Victory (1957) → R4VziGmpotQ\n",
      "✅ Found: Man Called Otto, A → eFYUX9l-m5I\n",
      "✅ Found: M3GAN → BRb4U99OU80\n",
      "✅ Found: Plane → M25zXBIUVr0\n",
      "✅ Found: Design for Living (1933) → Dk2xjxgryDc\n",
      "✅ Found: Babylon → 5muQK7CuFtY\n",
      "✅ Found: Terrifier 2 → 6KkONLf_ZKU\n",
      "✅ Found: Operation Fortune: Ruse de Guerre → WdZ-BWWQcWQ\n",
      "✅ Found: Maybe I Do → Z11QkNvMZpc\n",
      "✅ Found: Son, The → SJWRY4DzoAQ\n",
      "✅ Found: Out Of The Past (1947) → bMgGuCIZnsM\n",
      "✅ Found: They Drive by Night → Hd0VNz_zPO0\n",
      "✅ Found: Man From Laramie, The → pTISRce_XsY\n",
      "✅ Found: Knock at the Cabin → 0wiBHEACNHs\n",
      "✅ Found: Inspection, The → owkpFHP13R8\n",
      "✅ Found: Hot Seat → twQePz-lkXE\n",
      "✅ Found: Magic Mike's Last Dance → pBIGdw-BRxw\n",
      "✅ Found: TÁR → Na6gA1RehsU\n",
      "✅ Found: Titanic 25 Year Anniversary → oHY7D7K58BM\n",
      "✅ Found: Ant-Man and the Wasp: Quantumania → 5WfTEZJnv_8\n",
      "✅ Found: To Leslie → 2XOFxJjimAk\n",
      "✅ Found: Missing (2023) → seBixtcx19E\n",
      "✅ Found: Cocaine Bear → DuWEEKeJLMI\n",
      "✅ Found: Rear Window (1954) → R1Av5y_VUdQ\n",
      "✅ Found: Fantasia (re) '90 → bMe15j1eRc0\n",
      "✅ Found: Creed III → AHmCH7iB_IM\n",
      "✅ Found: Whale, The → nWiQodhMvz4\n",
      "✅ Found: Scream VI → h74AXqw4Opc\n",
      "✅ Found: 65 → bHXejJq5vr0\n",
      "✅ Found: Paws of Fury: The Legend of Hank → A_hkjvjx2ek\n",
      "✅ Found: Women Talking → pD0mFhMqDCE\n",
      "✅ Found: Savage Salvation → 67mj224y6hw\n",
      "✅ Found: Ambush → bvcacc5eLUY\n",
      "✅ Found: Shazam! Fury Of The Gods → Zi88i4CpHe4\n",
      "✅ Found: Psycho (1960) (re) → x4E68ShSTQk\n",
      "✅ Found: John Wick: Chapter 4 → qEVUtrk8_B4\n",
      "✅ Found: Till → NXd9DArUmCU\n",
      "✅ Found: Call Jane → U-1pUnTsSQc\n",
      "✅ Found: Pulp Fiction (1994) → tGpTpVyI_OQ\n",
      "✅ Found: Lilith (1964) → 0b86Ed1xbVM\n",
      "✅ Done scraping video IDs for batch 10.\n"
     ]
    }
   ],
   "source": [
    "####### batch 10\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_10_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_10_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 10.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bce137-ae40-4801-9a80-92ea22d1eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_10_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b49fb17-bdc2-4439-b07f-4a1213d234a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f869e625-707f-4547-8ac7-6ba0ca07214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e7cfc98-56b7-4716-bd82-1def043d3cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Avatar: The Way of Water\n",
      "Scraping: Fabelmans, The\n",
      "Scraping: Whitney Houston: I Wanna Dance with Somebody\n",
      "Scraping: Rock Dog 3 Battle the Beat\n",
      "Scraping: North By Northwest (1959)\n",
      "Scraping: Fortune Cookie, The\n",
      "Scraping: Angel (1937)\n",
      "Scraping: Shotgun Wedding\n",
      "Scraping: Bed Rest\n",
      "Scraping: Wrong Place\n",
      "Scraping: Bitter Victory (1957)\n",
      "Scraping: Man Called Otto, A\n",
      "Scraping: M3GAN\n",
      "Scraping: Plane\n",
      "Scraping: Design for Living (1933)\n",
      "Scraping: Babylon\n",
      "Scraping: Terrifier 2\n",
      "Scraping: Operation Fortune: Ruse de Guerre\n",
      "Scraping: Maybe I Do\n",
      "Scraping: Son, The\n",
      "Scraping: Out Of The Past (1947)\n",
      "Scraping: They Drive by Night\n",
      "Scraping: Man From Laramie, The\n",
      "Scraping: Knock at the Cabin\n",
      "Scraping: Inspection, The\n",
      "Scraping: Hot Seat\n",
      "Scraping: Magic Mike's Last Dance\n",
      "Scraping: TÁR\n",
      "Scraping: Titanic 25 Year Anniversary\n",
      "Scraping: Ant-Man and the Wasp: Quantumania\n",
      "Scraping: To Leslie\n",
      "Scraping: Missing (2023)\n",
      "Scraping: Cocaine Bear\n",
      "Scraping: Rear Window (1954)\n",
      "Scraping: Fantasia (re) '90\n",
      "Scraping: Creed III\n",
      "Scraping: Whale, The\n",
      "Scraping: Scream VI\n",
      "Scraping: 65\n",
      "Scraping: Paws of Fury: The Legend of Hank\n",
      "Scraping: Women Talking\n",
      "Scraping: Savage Salvation\n",
      "Scraping: Ambush\n",
      "Scraping: Shazam! Fury Of The Gods\n",
      "Scraping: Psycho (1960) (re)\n",
      "Scraping: John Wick: Chapter 4\n",
      "Scraping: Till\n",
      "Scraping: Call Jane\n",
      "Scraping: Pulp Fiction (1994)\n",
      "Scraping: Lilith (1964)\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch10 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch10.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d98fe1d9-87ce-4cd6-9825-231afd39bdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 115265 comments for batch 10.\n"
     ]
    }
   ],
   "source": [
    "comments_df_10 = pd.DataFrame(all_comments_batch10)\n",
    "comments_df_10.to_csv(\"comments_batch_10.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_10)} comments for batch 10.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3676028-d220-4d07-ac48-82b0a35b7093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Scarface (1983) → ptQd6a9MbPA\n",
      "✅ Found: Bigger Than Life → ros2nu77iRc\n",
      "✅ Found: Dungeons & Dragons: Honor Among Thieves → IiMinixSXII\n",
      "✅ Found: White Elephant → 5zEPU7Ekz00\n",
      "✅ Found: All the Beauty and the Bloodshed → kTKL5P_69e8\n",
      "✅ Found: Super Mario Bros. Movie, The → TnGl01FkMMo\n",
      "✅ Found: Lust For Life (1957) → c0Odg-GY40w\n",
      "✅ Found: Pope's Exorcist, The → YJXqvnT_rsk\n",
      "✅ Found: Beautiful Disaster (Vertical) → EMK3wy2563g\n",
      "✅ Found: Air (2023) → Euy4Yu6B3nU\n",
      "✅ Found: Metallica: 72 Seasons - Global Premiere → vkLxCFK5TyA\n",
      "✅ Found: Blade Runner (1982) → gCcx85zbxz4\n",
      "✅ Found: Once Upon A Time In America (1984) → PkXIl0-n-AY\n",
      "✅ Found: Exodus (1961) → KBzbyIPWVT4\n",
      "✅ Found: Miracle Worker, The → 0XunbqzGpAI\n",
      "✅ Found: Evil Dead Rise → smTK_AeAPHs\n",
      "✅ Found: 2001: A Space Odyssey (Re: 2014) → XHjIqQBsPjk\n",
      "✅ Found: War and Peace (1956) → WJdaWKww60g\n",
      "✅ Found: Citizen Kane (1941) → unOu9cyCZok\n",
      "✅ Found: Cleopatra (1963) → hX9fxdDWRHA\n",
      "✅ Found: Gone With The Wind (1939) → sbfSIWnWuU0\n",
      "✅ Found: Renfield → 6LmO6rmDW08\n",
      "✅ Found: On the Line (2022) → 0-1Yj5AlDcg\n",
      "✅ Found: Guardians of the Galaxy Vol. 3 → Q47BYO7D4yw\n",
      "✅ Found: Beau is Afraid → PuiWDn976Ek\n",
      "✅ Found: Book Club: The Next Chapter → i9k213d5FU0\n",
      "✅ Found: Love Again → CQDXtD2HJAs\n",
      "✅ Found: Jeepers Creepers Reborn → eNH2bRZ6gJw\n",
      "✅ Found: Machine Gun Kelly: Mainstream Sellout Live From Cleveland → DXhLwM5yBnA\n",
      "✅ Found: Fast X → VVbIrBSlBvo\n",
      "✅ Found: Godfather: Part II (Re: 2014), The → MJadTdd5SKA\n",
      "✅ Found: Godfather Part III, The (1990) (re) → Td8eEM9KDig\n",
      "✅ Found: Lost In Translation (2003) → SD1EB0dio34\n",
      "✅ Found: Little Mermaid, The → kpGo2_d3oYE\n",
      "✅ Found: Knights of the Zodiac → Sko0o_KoBHY\n",
      "✅ Found: Godfather, The (re) → TFyU-GzhKaM\n",
      "✅ Found: Raging Bull (1980) → cmROy7GhOZU\n",
      "✅ Found: TCM Presents E.T. The Extra-Terrestrial 35th Anniversary → xtGhsTAUudM\n",
      "✅ Found: Magnolia → KnamcFv_N9Q\n",
      "✅ Found: Bridges of Madison County, The → wJxkFKV_hyc\n",
      "✅ Found: Spider-Man: Across The Spider-Verse → shW9i6k8cB0\n",
      "✅ Found: Boogeyman, The → cFqCmIU0-_M\n",
      "✅ Found: Beetlejuice (1988) → ickbVzajrk0\n",
      "✅ Found: Transformers: Rise of the Beasts → itnqEauWQZM\n",
      "✅ Found: Marlowe → brVF7t8VYuM\n",
      "✅ Found: Interview With the Vampire (1994) → q8Oysn9I9K8\n",
      "✅ Found: Flash, The → hebWYacbdvc\n",
      "✅ Found: About My Father → txLSE7tpgr0\n",
      "✅ Found: Abyzou → hc7Q8NKjFk8\n",
      "✅ Found: Wendy and Lucy (2008) (re) → 2YRXHWxxABQ\n",
      "✅ Done scraping video IDs for batch 11.\n"
     ]
    }
   ],
   "source": [
    "########## batch 11\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_11_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_11_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 11.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2621c25f-033b-4b2e-8bcf-73540e730728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_11_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc5960b-97ec-4478-a510-862006b876c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcbf1d17-971d-48ae-8dd0-69da73513058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4eaad55-dbad-40e7-b482-257c88ca0234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Scarface (1983)\n",
      "Scraping: Bigger Than Life\n",
      "Scraping: Dungeons & Dragons: Honor Among Thieves\n",
      "Scraping: White Elephant\n",
      "Scraping: All the Beauty and the Bloodshed\n",
      "Scraping: Super Mario Bros. Movie, The\n",
      "Scraping: Lust For Life (1957)\n",
      "Scraping: Pope's Exorcist, The\n",
      "Scraping: Beautiful Disaster (Vertical)\n",
      "Scraping: Air (2023)\n",
      "Scraping: Metallica: 72 Seasons - Global Premiere\n",
      "Scraping: Blade Runner (1982)\n",
      "Scraping: Once Upon A Time In America (1984)\n",
      "Scraping: Exodus (1961)\n",
      "Scraping: Miracle Worker, The\n",
      "Scraping: Evil Dead Rise\n",
      "Scraping: 2001: A Space Odyssey (Re: 2014)\n",
      "Scraping: War and Peace (1956)\n",
      "Scraping: Citizen Kane (1941)\n",
      "Scraping: Cleopatra (1963)\n",
      "Scraping: Gone With The Wind (1939)\n",
      "Scraping: Renfield\n",
      "Scraping: On the Line (2022)\n",
      "Scraping: Guardians of the Galaxy Vol. 3\n",
      "Scraping: Beau is Afraid\n",
      "Scraping: Book Club: The Next Chapter\n",
      "Scraping: Love Again\n",
      "Scraping: Jeepers Creepers Reborn\n",
      "Scraping: Machine Gun Kelly: Mainstream Sellout Live From Cleveland\n",
      "Scraping: Fast X\n",
      "Scraping: Godfather: Part II (Re: 2014), The\n",
      "Scraping: Godfather Part III, The (1990) (re)\n",
      "Scraping: Lost In Translation (2003)\n",
      "Scraping: Little Mermaid, The\n",
      "Scraping: Knights of the Zodiac\n",
      "Scraping: Godfather, The (re)\n",
      "Scraping: Raging Bull (1980)\n",
      "Scraping: TCM Presents E.T. The Extra-Terrestrial 35th Anniversary\n",
      "Scraping: Magnolia\n",
      "Scraping: Bridges of Madison County, The\n",
      "Scraping: Spider-Man: Across The Spider-Verse\n",
      "Scraping: Boogeyman, The\n",
      "Scraping: Beetlejuice (1988)\n",
      "Scraping: Transformers: Rise of the Beasts\n",
      "Scraping: Marlowe\n",
      "Scraping: Interview With the Vampire (1994)\n",
      "Scraping: Flash, The\n",
      "Scraping: About My Father\n",
      "Scraping: Abyzou\n",
      "Scraping: Wendy and Lucy (2008) (re)\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch11 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch11.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d069a88a-fdb9-4eba-9951-ea91c33ac2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 93361 comments for batch 11.\n"
     ]
    }
   ],
   "source": [
    "comments_df_11 = pd.DataFrame(all_comments_batch11)\n",
    "comments_df_11.to_csv(\"comments_batch_11.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_11)} comments for batch 11.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b35f4e0-ef88-493f-afd2-4bc7666392b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Harold and Maude (re: 2012) → GwDpY6g5h-Y\n",
      "✅ Found: No Hard Feelings → P15S6ND8kbQ\n",
      "✅ Found: Asteroid City → 9FXCSXuGTF4\n",
      "✅ Found: Wrath of Becky, The → Q-RnvQACrH0\n",
      "✅ Found: Grand Budapest Hotel, The (2014) (re) → V-aWDKWhPmw\n",
      "✅ Found: Never Rarely Sometimes Always → 7SApLTmbtvg\n",
      "✅ Found: Esther Newton Made Me Gay → sTak3GWWXO8\n",
      "✅ Found: Indiana Jones and the Dial of Destiny → HX8ROmTlds8\n",
      "✅ Found: Ruby Gillman, Teenage Kraken → iAMKwDsfUeU\n",
      "✅ Found: Cop Land (1997) → D3HSm83eXgo\n",
      "✅ Found: Before You Know It (2014) → 6W-8MDaccpM\n",
      "✅ Found: Walk the Line (2005) (re) (Spain) → pbQ22zWPYbw\n",
      "✅ Found: Ziggy Stardust: 50th Anniversary Event → Sc39uFmVQB4\n",
      "✅ Found: Insidious: The Red Door → ZuQuOnYnr3Q\n",
      "✅ Found: Black Demon, The → z1xJAyVKAPY\n",
      "✅ Found: One True Loves → 87EeZG2n2po\n",
      "✅ Found: Mission: Impossible - Dead Reckoning Part One → avz06PDqDbM\n",
      "✅ Found: Elemental → hXzcyx9V0xw\n",
      "✅ Found: Thing, The (1982) → SFaTFOUz9gc\n",
      "✅ Found: Cry Baby → OtdqC22tfv0\n",
      "✅ Found: Rock Bottom Riser → dmO6B_s-KMk\n",
      "✅ Found: I Am Sam → hCszp54Qcoc\n",
      "✅ Found: Barbie → pBk4NYhWNMM\n",
      "✅ Found: Oppenheimer → uYPbbksJxIg\n",
      "✅ Found: Torn Curtain (1966) → ZwtiA4T7VlM\n",
      "✅ Found: Hell and High Water (1954) (re) → P1zyh_whgVQ\n",
      "✅ Found: Haunted Mansion → AjLKTz81bj8\n",
      "✅ Found: Cobweb → hGY0icwlDGY\n",
      "✅ Found: Meg 2: The Trench → dG91B3hHyY4\n",
      "✅ Found: Gran Turismo: Based on a True Story → GVPzGBvPrzw\n",
      "✅ Found: Teenage Mutant Ninja Turtles: Mutant Mayhem → IHvzw4Ibuho\n",
      "✅ Found: Last Voyage of the Demeter, The → 6FgUUO9Ztd0\n",
      "✅ Found: See You on Venus → 8l3vQ7cyqeY\n",
      "✅ Found: Two Weeks In Another Town → sgY2L7NdGN0\n",
      "✅ Found: Roman Holiday (1953) → 2vm2FdHg7Io\n",
      "✅ Found: Blue Beetle → vS3_72Gb-bI\n",
      "✅ Found: Strays → BzwOLKTbCUw\n",
      "✅ Found: Metallica M72 World Tour Live from TX #1 → ieEWQjbQ7V8\n",
      "✅ Found: Metallica M72 World Tour Live from TX #2 → -frmj62OUV4\n",
      "✅ Found: Silk Road Rally, The → 6S1oRHIpV3w\n",
      "✅ Found: Equalizer 3, The → 19ikl8vy4zs\n",
      "✅ Found: Good Person, A → phRXBLwcy5I\n",
      "✅ Found: Exorcist, The (1973) → PIxpPMyGcpU\n",
      "✅ Found: Grapes of Wrath → JzOIRcYj8dE\n",
      "✅ Found: Nun II, The → QF-oyCwaArU\n",
      "✅ Found: Retribution → jzQn0-WH4WM\n",
      "✅ Found: To Live and Die in L.A. → _tq018xnM7w\n",
      "✅ Found: Stop Making Sense Remastered → -rjMwSTeVeo\n",
      "✅ Found: After Everything (Event) → smavD1fopZ8\n",
      "✅ Found: Haunting in Venice, A → yEddsSwweyE\n",
      "✅ Done scraping video IDs for batch 12.\n"
     ]
    }
   ],
   "source": [
    "####### batch 12\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_12_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_12_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 12.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f11d6d69-1be5-4eb3-a9dd-9e4db14beb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_12_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5d108d6-8544-4966-a196-d0dd5d3883dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aff5405-5cc0-4d11-a22c-50ea00415f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a69d66-fc41-4556-aa4a-df7411792614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Harold and Maude (re: 2012)\n",
      "Scraping: No Hard Feelings\n",
      "Scraping: Asteroid City\n",
      "Scraping: Wrath of Becky, The\n",
      "Scraping: Grand Budapest Hotel, The (2014) (re)\n",
      "Scraping: Never Rarely Sometimes Always\n",
      "Scraping: Esther Newton Made Me Gay\n",
      "Scraping: Indiana Jones and the Dial of Destiny\n",
      "Scraping: Ruby Gillman, Teenage Kraken\n",
      "Scraping: Cop Land (1997)\n",
      "Scraping: Before You Know It (2014)\n",
      "Scraping: Walk the Line (2005) (re) (Spain)\n",
      "Scraping: Ziggy Stardust: 50th Anniversary Event\n",
      "Scraping: Insidious: The Red Door\n",
      "Scraping: Black Demon, The\n",
      "Scraping: One True Loves\n",
      "Scraping: Mission: Impossible - Dead Reckoning Part One\n",
      "Scraping: Elemental\n",
      "Scraping: Thing, The (1982)\n",
      "Scraping: Cry Baby\n",
      "Scraping: Rock Bottom Riser\n",
      "Scraping: I Am Sam\n",
      "Scraping: Barbie\n",
      "Scraping: Oppenheimer\n",
      "Scraping: Torn Curtain (1966)\n",
      "Scraping: Hell and High Water (1954) (re)\n",
      "Scraping: Haunted Mansion\n",
      "Scraping: Cobweb\n",
      "Scraping: Meg 2: The Trench\n",
      "Scraping: Gran Turismo: Based on a True Story\n",
      "Scraping: Teenage Mutant Ninja Turtles: Mutant Mayhem\n",
      "Scraping: Last Voyage of the Demeter, The\n",
      "Scraping: See You on Venus\n",
      "Scraping: Two Weeks In Another Town\n",
      "Scraping: Roman Holiday (1953)\n",
      "Scraping: Blue Beetle\n",
      "Scraping: Strays\n",
      "Scraping: Metallica M72 World Tour Live from TX #1\n",
      "Scraping: Metallica M72 World Tour Live from TX #2\n",
      "Scraping: Silk Road Rally, The\n",
      "Scraping: Equalizer 3, The\n",
      "Scraping: Good Person, A\n",
      "Scraping: Exorcist, The (1973)\n",
      "Scraping: Grapes of Wrath\n",
      "Scraping: Nun II, The\n",
      "Scraping: Retribution\n",
      "Scraping: To Live and Die in L.A.\n",
      "Scraping: Stop Making Sense Remastered\n",
      "Scraping: After Everything (Event)\n",
      "Scraping: Haunting in Venice, A\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch12 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch12.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f535abd4-5414-4862-aa68-e4eb3a35c05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 121592 comments for batch 12.\n"
     ]
    }
   ],
   "source": [
    "comments_df_12 = pd.DataFrame(all_comments_batch12)\n",
    "comments_df_12.to_csv(\"comments_batch_12.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_12)} comments for batch 12.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ed45d0-5512-49ff-9952-ba3c80dc43da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: French Connection, The (1971) → XjCR94HR2g8\n",
      "✅ Found: Cruising (re) → avz06PDqDbM\n",
      "✅ Found: Ghostbusters (re: 2014) → ahZFCF--uRY\n",
      "✅ Found: Expend4bles → DhlaBO-SwVE\n",
      "✅ Found: Dumb Money → bmr8YmwnZ3w\n",
      "❌ Error for 'Ted K': list index out of range\n",
      "✅ Found: Carlos → Vj_kMqw8i3A\n",
      "✅ Found: Paw Patrol: The Mighty Movie → UDgjJ9XzgHk\n",
      "✅ Found: Creator, The → ex3C1-5Dhb8\n",
      "✅ Found: Blood → TZX6E0M47Q8\n",
      "✅ Found: Exorcist: Believer, The → PIxpPMyGcpU\n",
      "✅ Found: Misanthrope → DckI7n1_q2Q\n",
      "✅ Found: Man Who Shot Liberty Valance, The → Z3cR7XPQUzc\n",
      "✅ Found: Young Mr. Lincoln → 9JDOVrOPHzA\n",
      "✅ Found: Robinson Crusoe (1954) → NcEOr64cSzI\n",
      "✅ Found: 57 Seconds → L8XsJ4O1YJ0\n",
      "✅ Found: Whole Town's Talking, The → jXChkfCpN2A\n",
      "✅ Found: TAYLOR SWIFT THE ERAS TOUR → KudedLV0tP0\n",
      "✅ Found: Caged (1950) → yzJ2S2_S4T0\n",
      "✅ Found: Killers of the Flower Moon → 7cx9nCHsemc\n",
      "✅ Found: Hunger Games (2023 Event), The → MjFhOwKitFM\n",
      "✅ Found: Perpetrator → _KZLh-FXjKU\n",
      "✅ Found: NYAD → 3anCgVSQb3Q\n",
      "✅ Found: Trolls Band Together → qZ40Z62tcXM\n",
      "✅ Found: Saw X → t3PzUo4P21c\n",
      "✅ Found: How Green Was My Valley → ktjwzgwRZQ4\n",
      "✅ Found: My Darling Clementine → oQA33nAbuiU\n",
      "✅ Found: Killer, The (2023) → 5S7FR_HCg9g\n",
      "✅ Found: Five Nights at Freddy's → 0VH9WCFV6XQ\n",
      "✅ Found: Kandahar → WHs6z9RPGtA\n",
      "✅ Found: Marsh King's Daughter, The → FDnUVhLMqI0\n",
      "✅ Found: She Came to Me → ZCbiKiw-Azs\n",
      "✅ Found: Travels with My Aunt (1972) (re) → ypIbTpnuNgg\n",
      "✅ Found: Big Lebowski, The (re) → c57cfPO7rBE\n",
      "✅ Found: Marvels, The → 8NCsKgNcpEE\n",
      "✅ Found: It Lives Inside → a5xUbuYHdi8\n",
      "✅ Found: Dolly Parton ROCKSTAR: The Global First Listen Event → _qi5fR5NScE\n",
      "✅ Found: Hunger Games: The Ballad of Songbirds & Snakes, The → RDE6Uz73A7g\n",
      "✅ Found: Freelance → BrqWlOzm2Iw\n",
      "✅ Found: Master Gardener → ZzD5Rx-yhO8\n",
      "✅ Found: Wish → ctlz0R1tSZE\n",
      "✅ Found: Napoleon → OAZWXUkrjPc\n",
      "✅ Found: Thanksgiving → KbU50SdL8zA\n",
      "✅ Found: May December → _2wg45bRRF8\n",
      "✅ Found: Streetcar Named Desire, A (1951) (re) → A5SLQ4cHAwY\n",
      "✅ Found: Tobacco Road → c2TV7V_koGc\n",
      "✅ Found: RENAISSANCE: A FILM BY BEYONCE → BGYIXarTNbA\n",
      "✅ Found: Casablanca (1942) (re) → dvWZzS6IbS8\n",
      "✅ Found: King Of Comedy (1983) → BOpbikZm3JY\n",
      "✅ Found: Wonka → otNh9bTjXWg\n",
      "✅ Done scraping video IDs for batch 13.\n"
     ]
    }
   ],
   "source": [
    "######## batch 13\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_13_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_13_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 13.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91c99f1-858a-4f9d-98cd-ae9b07fc0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_13_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce28b993-802d-4b5d-9086-08a0eead02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e524d64-c10e-4ebe-928c-7c0fe4225789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9754ec67-3e59-4b5c-9f13-1f0d87c708f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: French Connection, The (1971)\n",
      "Scraping: Cruising (re)\n",
      "Scraping: Ghostbusters (re: 2014)\n",
      "Scraping: Expend4bles\n",
      "Scraping: Dumb Money\n",
      "Scraping: Carlos\n",
      "Scraping: Paw Patrol: The Mighty Movie\n",
      "Scraping: Creator, The\n",
      "Scraping: Blood\n",
      "Scraping: Exorcist: Believer, The\n",
      "Scraping: Misanthrope\n",
      "Scraping: Man Who Shot Liberty Valance, The\n",
      "Scraping: Young Mr. Lincoln\n",
      "Scraping: Robinson Crusoe (1954)\n",
      "Scraping: 57 Seconds\n",
      "Scraping: Whole Town's Talking, The\n",
      "Scraping: TAYLOR SWIFT THE ERAS TOUR\n",
      "Scraping: Caged (1950)\n",
      "Scraping: Killers of the Flower Moon\n",
      "Scraping: Hunger Games (2023 Event), The\n",
      "Scraping: Perpetrator\n",
      "Scraping: NYAD\n",
      "Scraping: Trolls Band Together\n",
      "Scraping: Saw X\n",
      "Scraping: How Green Was My Valley\n",
      "Scraping: My Darling Clementine\n",
      "Scraping: Killer, The (2023)\n",
      "Scraping: Five Nights at Freddy's\n",
      "Scraping: Kandahar\n",
      "Scraping: Marsh King's Daughter, The\n",
      "Scraping: She Came to Me\n",
      "Scraping: Travels with My Aunt (1972) (re)\n",
      "Scraping: Big Lebowski, The (re)\n",
      "Scraping: Marvels, The\n",
      "Scraping: It Lives Inside\n",
      "Scraping: Dolly Parton ROCKSTAR: The Global First Listen Event\n",
      "Scraping: Hunger Games: The Ballad of Songbirds & Snakes, The\n",
      "Scraping: Freelance\n",
      "Scraping: Master Gardener\n",
      "Scraping: Wish\n",
      "Scraping: Napoleon\n",
      "Scraping: Thanksgiving\n",
      "Scraping: May December\n",
      "Scraping: Streetcar Named Desire, A (1951) (re)\n",
      "Scraping: Tobacco Road\n",
      "Scraping: RENAISSANCE: A FILM BY BEYONCE\n",
      "Scraping: Casablanca (1942) (re)\n",
      "Scraping: King Of Comedy (1983)\n",
      "Scraping: Wonka\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch13 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch13.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d660bbd-e0ec-4b5e-9d04-fb52fef2c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 140815 comments for batch 13.\n"
     ]
    }
   ],
   "source": [
    "comments_df_13 = pd.DataFrame(all_comments_batch13)\n",
    "comments_df_13.to_csv(\"comments_batch_13.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_13)} comments for batch 13.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e478ab-43fa-4a60-b6e8-baeb722f5804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Hypnotic → N-qn4h-amyY\n",
      "✅ Found: Night of the Iguana (1964) → 6DHgJ5eikaI\n",
      "✅ Found: Seven Women → pD0mFhMqDCE\n",
      "✅ Found: Maestro → gJP2QblqLA0\n",
      "✅ Found: Migration → ItiZ_j-tu0I\n",
      "✅ Found: Silent Night → yBnTqn0lBDA\n",
      "✅ Found: All Fun and Games → dI7n7So6sVA\n",
      "✅ Found: Fool's Paradise → RaYKG5FMhNc\n",
      "✅ Found: Aquaman and the Lost Kingdom → UGc5Tzz19UY\n",
      "✅ Found: Sound of Freedom → Rt0kp4VW1cI\n",
      "✅ Found: It's a Wonderful Knife → qs4EOJKfFeE\n",
      "✅ Found: Sociedad de la nieve, La → l9tP4M8URhQ\n",
      "✅ Found: Anyone But You → ESEc6W2h9_k\n",
      "✅ Found: Next Goal Wins → pRH5u5lpArQ\n",
      "✅ Found: Doctor Dolittle → bi1BbL9HinM\n",
      "✅ Found: Ferrari → KZHxT2yb2cE\n",
      "✅ Found: Night Swim → pcSNqteCEtE\n",
      "✅ Found: This Property Is Condemned → zyREPrVRkoI\n",
      "✅ Found: Fugitive Kind, The → AKQjnY0ZHJM\n",
      "✅ Found: Mean Girls → fFtdbEgnUOk\n",
      "✅ Found: Beekeeper, The → SzINZZ6iqxY\n",
      "✅ Found: White Bird → aTTPea6gHh4\n",
      "✅ Found: Poor Things → _klfx5sGzFk\n",
      "✅ Found: Beautiful Wedding → xO1FZJQmiAo\n",
      "✅ Found: Kill Room, The → ORdhNaXSrGA\n",
      "✅ Found: Retirement Plan, The → i2Xc4qq-5Ww\n",
      "✅ Found: On Fire → MyOjzLEnxk8\n",
      "✅ Found: Splendor In The Grass → 1lN2jgJRVzI\n",
      "✅ Found: Argylle → 7mgu9mNZ8Hk\n",
      "✅ Found: Supercell → -22ISMHPK4A\n",
      "✅ Found: Some Came Running (1958) → Qv0kz_NlsHQ\n",
      "✅ Found: Past Lives → kA244xewjcI\n",
      "✅ Found: Color Purple, The → wPwzBUui1GA\n",
      "✅ Found: Consecration → HRjG65M6L2c\n",
      "✅ Found: Bob Marley: One Love → ajw425Kuvtw\n",
      "✅ Found: Madame Web → s_76M4c4LTo\n",
      "✅ Found: Holdovers, The → AhKLpJmHhIg\n",
      "✅ Found: Favourite, The (2018) (re) → bnsKXB_5YIY\n",
      "✅ Found: Iron Claw, The → 8KVsaoveTbw\n",
      "✅ Found: Birth (2004) → 3mZq5Cgey9A\n",
      "✅ Found: Dune: Part Two → Way9Dexny3w\n",
      "✅ Found: Imaginary → 8XoNfrgrAGM\n",
      "✅ Found: Priscilla → DBWk6BohVXk\n",
      "✅ Found: No Way Up → PSEoAaSswgo\n",
      "✅ Found: Kung Fu Panda 4 → _inKs4eeHiI\n",
      "✅ Found: To Kill a Mockingbird (re 2010) → 5MTauicV2CI\n",
      "✅ Found: Ghostbusters: Frozen Empire → HpOBXh02rVc\n",
      "✅ Found: Arthur the King → wjDJNEPghNY\n",
      "✅ Found: Bricklayer, The → ayPesvG86gE\n",
      "❌ Error for 'Piper, The (dir. Thoroddsen)': list index out of range\n",
      "✅ Done scraping video IDs for batch 14.\n"
     ]
    }
   ],
   "source": [
    "####### batch 14\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_14_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_14_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 14.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e8716a-f28e-472c-b221-e06b1e9cbd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_14_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c01c217-17ca-43e6-b6fc-5bbb41dcb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb34e154-2607-460e-a91d-6856fabf07eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3a1747-ef00-416b-87f4-0b65ff07295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Hypnotic\n",
      "Scraping: Night of the Iguana (1964)\n",
      "Scraping: Seven Women\n",
      "Scraping: Maestro\n",
      "Scraping: Migration\n",
      "Scraping: Silent Night\n",
      "Scraping: All Fun and Games\n",
      "Scraping: Fool's Paradise\n",
      "Scraping: Aquaman and the Lost Kingdom\n",
      "Scraping: Sound of Freedom\n",
      "Scraping: It's a Wonderful Knife\n",
      "Scraping: Sociedad de la nieve, La\n",
      "Scraping: Anyone But You\n",
      "Scraping: Next Goal Wins\n",
      "Scraping: Doctor Dolittle\n",
      "Scraping: Ferrari\n",
      "Scraping: Night Swim\n",
      "Scraping: This Property Is Condemned\n",
      "Scraping: Fugitive Kind, The\n",
      "Scraping: Mean Girls\n",
      "Scraping: Beekeeper, The\n",
      "Scraping: White Bird\n",
      "Scraping: Poor Things\n",
      "Scraping: Beautiful Wedding\n",
      "Scraping: Kill Room, The\n",
      "Scraping: Retirement Plan, The\n",
      "Scraping: On Fire\n",
      "Scraping: Splendor In The Grass\n",
      "Scraping: Argylle\n",
      "Scraping: Supercell\n",
      "Scraping: Some Came Running (1958)\n",
      "Scraping: Past Lives\n",
      "Scraping: Color Purple, The\n",
      "Scraping: Consecration\n",
      "Scraping: Bob Marley: One Love\n",
      "Scraping: Madame Web\n",
      "Scraping: Holdovers, The\n",
      "Scraping: Favourite, The (2018) (re)\n",
      "Scraping: Iron Claw, The\n",
      "Scraping: Birth (2004)\n",
      "Scraping: Dune: Part Two\n",
      "Scraping: Imaginary\n",
      "Scraping: Priscilla\n",
      "Scraping: No Way Up\n",
      "Scraping: Kung Fu Panda 4\n",
      "Scraping: To Kill a Mockingbird (re 2010)\n",
      "Scraping: Ghostbusters: Frozen Empire\n",
      "Scraping: Arthur the King\n",
      "Scraping: Bricklayer, The\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch14 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch14.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190db74d-af41-4a3d-b439-e882e2d96492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 59564 comments for batch 14.\n"
     ]
    }
   ],
   "source": [
    "comments_df_14 = pd.DataFrame(all_comments_batch14)\n",
    "comments_df_14.to_csv(\"comments_batch_14.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_14)} comments for batch 14.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e627d71-c0fb-49e5-87ee-b82224b49d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Lady From Shanghai, The → C_FTmZ0Y_hw\n",
      "✅ Found: Quiet American, The (1958) (re) → RPf2D-N7sSs\n",
      "✅ Found: Forty Guns → 84ll1Dcu0rY\n",
      "✅ Found: Godzilla x Kong: The New Empire → lV1OOlGwExM\n",
      "✅ Found: Immaculate → ewxS9Z-XXYo\n",
      "✅ Found: Ordinary Angels → R1vn8kPgCYA\n",
      "✅ Found: Strangers On A Train (1951) (re) → jrDxOb7auk4\n",
      "✅ Found: Marnie (1964) → ocC0xfqP2RQ\n",
      "✅ Found: Where The Sidewalk Ends → CIucvAJNlSY\n",
      "✅ Found: Man of the West (1958) → 2r-eQoHyLsQ\n",
      "✅ Found: First Omen, The → lmN1Op8ygno\n",
      "✅ Found: Monkey Man → g8zxiB5Qhsc\n",
      "✅ Found: Miller's Girl → vk2OJZHutBM\n",
      "✅ Found: Sleeping Dogs → 9AFIutyHvKs\n",
      "✅ Found: Pearl Jam - Dark Matter - Global Theatrical Experience - One Night Only → sUQ87XiaUr4\n",
      "✅ Found: Civil War → aDyQxtg0V2w\n",
      "✅ Found: Cabrini → 02S9IcBPefw\n",
      "✅ Found: Inventor, The → Ez1NHnJrk2Y\n",
      "✅ Found: Sympathy for the Devil → ap8IKdWWHWw\n",
      "✅ Found: On The Waterfront → y0u9F4PEOaU\n",
      "❌ Error for 'Julius Caesar (1953)': list index out of range\n",
      "✅ Found: Second Civil War, The → aDyQxtg0V2w\n",
      "✅ Found: Challengers → VobTTbg-te0\n",
      "✅ Found: Reflections In A Golden Eye → 6nV9y6I84CA\n",
      "✅ Found: Chase, The (1966) → NxG4xv3thRA\n",
      "✅ Found: Missouri Breaks, The (1976) → lqbn_ixTdw4\n",
      "✅ Found: Fall Guy, The → j7jPnwVGdZ8\n",
      "✅ Found: Tarot → bvDArsKoTOE\n",
      "✅ Found: Mothers' Instinct → 4ltU9ooQ8x0\n",
      "✅ Found: Soft & Quiet → 9caXYrlADa0\n",
      "✅ Found: Mutiny On The Bounty (1962) → uJ3quGpqqNg\n",
      "✅ Found: Star Wars: Episode I - Phantom Menace (1999) (re) → oH96QbNKIcI\n",
      "✅ Found: Kingdom of the Planet of the Apes → XtFI7SNtVpY\n",
      "✅ Found: Abigail → 3PsP8MFH8p0\n",
      "✅ Found: Couple, Un → WouMCM1EsEE\n",
      "✅ Found: IF (2024) → mb2187ZQtBE\n",
      "✅ Found: Strangers: Chapter 1, The → 3pZUQmZdOi4\n",
      "✅ Found: Arcadian → HNARuSROxbM\n",
      "✅ Found: Girl in the Backseat, The → -KrWYiM5WWc\n",
      "✅ Found: Garfield Movie, The → IeFWNtMo1Fs\n",
      "✅ Found: Furiosa: A Mad Max Saga → XJMuhwVlca4\n",
      "✅ Found: Queen Mary → JJg9K716auA\n",
      "✅ Found: Shanghai Express (1932) → Nw6oPC2eghI\n",
      "✅ Found: M. Butterfly → ePw2msXf1Qs\n",
      "✅ Found: Hit Man → 0g4cJ4NE8HA\n",
      "✅ Found: Origin → ytUyrTQJp8M\n",
      "✅ Found: Children Of Men (2006) (re) → FmSwAGkuXDY\n",
      "✅ Found: Bad Boys: Ride or Die → hRFY_Fesa9Q\n",
      "✅ Found: Watchers, The → dYo91Fq9tKY\n",
      "✅ Found: Exorcism, The → wCe2QqMNexM\n",
      "✅ Done scraping video IDs for batch 15.\n"
     ]
    }
   ],
   "source": [
    "###### batch 15\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_15_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_15_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 15.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d3528fd-e9e2-4a95-850b-3cfe14f2a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_15_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9b29acb-828d-43c1-a678-215be9977dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3329ad24-7e54-47ff-9234-1516a79755ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefa2b72-8bd9-470d-ab15-65d9749ce0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Lady From Shanghai, The\n",
      "Scraping: Quiet American, The (1958) (re)\n",
      "Scraping: Forty Guns\n",
      "Scraping: Godzilla x Kong: The New Empire\n",
      "Scraping: Immaculate\n",
      "Scraping: Ordinary Angels\n",
      "Scraping: Strangers On A Train (1951) (re)\n",
      "Scraping: Marnie (1964)\n",
      "Scraping: Where The Sidewalk Ends\n",
      "Scraping: Man of the West (1958)\n",
      "Scraping: First Omen, The\n",
      "Scraping: Monkey Man\n",
      "Scraping: Miller's Girl\n",
      "Scraping: Sleeping Dogs\n",
      "Scraping: Pearl Jam - Dark Matter - Global Theatrical Experience - One Night Only\n",
      "Scraping: Civil War\n",
      "Scraping: Cabrini\n",
      "Scraping: Inventor, The\n",
      "Scraping: Sympathy for the Devil\n",
      "Scraping: On The Waterfront\n",
      "Scraping: Second Civil War, The\n",
      "Scraping: Challengers\n",
      "Scraping: Reflections In A Golden Eye\n",
      "Scraping: Chase, The (1966)\n",
      "Scraping: Missouri Breaks, The (1976)\n",
      "Scraping: Fall Guy, The\n",
      "Scraping: Tarot\n",
      "Scraping: Mothers' Instinct\n",
      "Scraping: Soft & Quiet\n",
      "Scraping: Mutiny On The Bounty (1962)\n",
      "Scraping: Star Wars: Episode I - Phantom Menace (1999) (re)\n",
      "Scraping: Kingdom of the Planet of the Apes\n",
      "Scraping: Abigail\n",
      "Scraping: Couple, Un\n",
      "Scraping: IF (2024)\n",
      "Scraping: Strangers: Chapter 1, The\n",
      "Scraping: Arcadian\n",
      "Scraping: Girl in the Backseat, The\n",
      "Scraping: Garfield Movie, The\n",
      "Scraping: Furiosa: A Mad Max Saga\n",
      "Scraping: Queen Mary\n",
      "Scraping: Shanghai Express (1932)\n",
      "Scraping: M. Butterfly\n",
      "Scraping: Hit Man\n",
      "Scraping: Origin\n",
      "Scraping: Children Of Men (2006) (re)\n",
      "Scraping: Bad Boys: Ride or Die\n",
      "Scraping: Watchers, The\n",
      "Scraping: Exorcism, The\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch15 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch15.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "963b876c-6d6b-4b68-8198-b3e5c103cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 118117 comments for batch 15.\n"
     ]
    }
   ],
   "source": [
    "comments_df_15 = pd.DataFrame(all_comments_batch15)\n",
    "comments_df_15.to_csv(\"comments_batch_15.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_15)} comments for batch 15.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd98db1-6ad3-4ee7-889e-88825d61cef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Dream Scenario → q3x9iUL-74w\n",
      "✅ Found: Little Women (2019) (re) → KlBRqoxEc8o\n",
      "✅ Found: To Catch A Thief (1955) → jahsI4VifPo\n",
      "✅ Found: Tetro (2009) (re) → 2NN5CRlcljY\n",
      "✅ Found: Bikeriders, The → BrSaVt5pvPk\n",
      "✅ Found: GHOST: RITE HERE RITE NOW → GJVgZX-vQUc\n",
      "✅ Found: Despicable Me 4 → qQlr9-rF32A\n",
      "✅ Found: Quiet Place: Day One, A → YPY7J-flzE8\n",
      "✅ Found: Sweet East, The → hJgNw9deqtc\n",
      "❌ Error for 'Touch Of Evil (1958) (re)': list index out of range\n",
      "✅ Found: Horizon: An American Saga Chapter 1 → YYsReoZMj1k\n",
      "✅ Found: One from the Heart: Reprise → G8OdSGjOBR4\n",
      "✅ Found: Repo Man → 2nKJhH8L9_w\n",
      "✅ Found: Inside Out 2 → LEjhY15eCx0\n",
      "✅ Found: Fly Me to the Moon → lW7enw6mFxs\n",
      "✅ Found: Freud's Last Session → hPJM9lEMyV4\n",
      "✅ Found: To Have And Have Not (1944) → DqSJxa3hT60\n",
      "✅ Found: Big Sleep, The (1946) → 5X5OamLAiEM\n",
      "✅ Found: Adventures Of Buckaroo Banzai Across The 8th Dimension, The → G93EgDoyKs4\n",
      "✅ Found: Twisters → wdok0rZdmx4\n",
      "✅ Found: Chief of Station → Krsu0mazuZ4\n",
      "✅ Found: Memory → A3sOCO8mhS8\n",
      "✅ Found: Today We Live → AuB9FF5qCSA\n",
      "✅ Found: Land of the Pharoahs → yaMrCsnjUss\n",
      "✅ Found: RoboCop (1987) → 44k3D5FhdcQ\n",
      "✅ Found: Deadpool & Wolverine → 9wV42pvAbAk\n",
      "✅ Found: Longlegs → OG7wOTE8NhE\n",
      "✅ Found: They Live → -5X2pt95cIo\n",
      "✅ Found: Trap → hJiPAJKjUVg\n",
      "✅ Found: Knox Goes Away → bZ8SeYVnc9A\n",
      "✅ Found: It Ends with Us → DLET_u31M4M\n",
      "✅ Found: Borderlands → lU_NKNZljoQ\n",
      "✅ Found: Four Horsemen Of The Apocalypse → HIRqMu_MHSI\n",
      "✅ Found: Bad And The Beautiful, The (re) → gMuQaq_hPGI\n",
      "✅ Found: Alien: Romulus → OzY2r2JXsDM\n",
      "✅ Found: Harold and the Purple Crayon → WojIv-PVYm8\n",
      "✅ Found: Coraline 15th Anniversary → T6iQnnHNF50\n",
      "✅ Found: Gracie & Pedro: Pets to the Rescue! → eqadP5cDBUg\n",
      "✅ Found: Home from the hill → 3eqxXqJDmcY\n",
      "✅ Found: Barefoot Contessa, The → x7iVp4H6X0k\n",
      "✅ Found: Blink Twice → aMcmfonGWY4\n",
      "✅ Found: Crow, The → djSKp_pwmOA\n",
      "✅ Found: In the Land of Saints and Sinners → lWwhYRxhngc\n",
      "✅ Found: Robots → DD4WBGptMSw\n",
      "✅ Found: Hellboy: The Crooked Man → 6UEO6QTbvKc\n",
      "✅ Found: Longing → cx0wrMWZd4g\n",
      "✅ Found: Deliver Us → hlEo8c68cQ0\n",
      "✅ Found: Don't Turn Out the Lights → 6RMxIBMppwY\n",
      "✅ Found: Play Dead → 72KaDGRKwYY\n",
      "✅ Found: Beetlejuice Beetlejuice → CoZqL9N6Rx4\n",
      "✅ Done scraping video IDs for batch 16.\n"
     ]
    }
   ],
   "source": [
    "######### batch 16\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_16_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_16_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87af6dce-8ed8-4a63-a4e0-5a7e90228062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_16_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55cad9c-f031-4ccd-8834-dfeab8440a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1add0ed1-4710-45e3-b8b1-9835fa6ecc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951e6472-0277-4e5d-ade7-c87a98963c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Dream Scenario\n",
      "Scraping: Little Women (2019) (re)\n",
      "Scraping: To Catch A Thief (1955)\n",
      "Scraping: Tetro (2009) (re)\n",
      "Scraping: Bikeriders, The\n",
      "Scraping: GHOST: RITE HERE RITE NOW\n",
      "Scraping: Despicable Me 4\n",
      "Scraping: Quiet Place: Day One, A\n",
      "Scraping: Sweet East, The\n",
      "Scraping: Horizon: An American Saga Chapter 1\n",
      "Scraping: One from the Heart: Reprise\n",
      "Scraping: Repo Man\n",
      "Scraping: Inside Out 2\n",
      "Scraping: Fly Me to the Moon\n",
      "Scraping: Freud's Last Session\n",
      "Scraping: To Have And Have Not (1944)\n",
      "Scraping: Big Sleep, The (1946)\n",
      "Scraping: Adventures Of Buckaroo Banzai Across The 8th Dimension, The\n",
      "Scraping: Twisters\n",
      "Scraping: Chief of Station\n",
      "Scraping: Memory\n",
      "Scraping: Today We Live\n",
      "Scraping: Land of the Pharoahs\n",
      "Scraping: RoboCop (1987)\n",
      "Scraping: Deadpool & Wolverine\n",
      "Scraping: Longlegs\n",
      "Scraping: They Live\n",
      "Scraping: Trap\n",
      "Scraping: Knox Goes Away\n",
      "Scraping: It Ends with Us\n",
      "Scraping: Borderlands\n",
      "Scraping: Four Horsemen Of The Apocalypse\n",
      "Scraping: Bad And The Beautiful, The (re)\n",
      "Scraping: Alien: Romulus\n",
      "Scraping: Harold and the Purple Crayon\n",
      "Scraping: Coraline 15th Anniversary\n",
      "Scraping: Gracie & Pedro: Pets to the Rescue!\n",
      "Scraping: Home from the hill\n",
      "Scraping: Barefoot Contessa, The\n",
      "Scraping: Blink Twice\n",
      "Scraping: Crow, The\n",
      "Scraping: In the Land of Saints and Sinners\n",
      "Scraping: Robots\n",
      "Scraping: Hellboy: The Crooked Man\n",
      "Scraping: Longing\n",
      "Scraping: Deliver Us\n",
      "Scraping: Don't Turn Out the Lights\n",
      "Scraping: Play Dead\n",
      "Scraping: Beetlejuice Beetlejuice\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch16 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch16.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0195e30-5a6d-405b-9c63-ad5e10313066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 72285 comments for batch 16.\n"
     ]
    }
   ],
   "source": [
    "comments_df_16 = pd.DataFrame(all_comments_batch16)\n",
    "comments_df_16.to_csv(\"comments_batch_16.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_16)} comments for batch 16.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ce8fa6-b025-495c-a60e-a867ff99dc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Land of Bad → yTFazxfrXVw\n",
      "✅ Found: Daddio → PJrr2amlFyc\n",
      "✅ Found: East of Eden → QQZ1Ti1g8gU\n",
      "✅ Found: Rebel Without A Cause (1955) → HWHH5TwEwtI\n",
      "✅ Found: Duck Soup → PjlmjL00VaA\n",
      "✅ Found: Ruggles of Red Gap (re) → Y5TZkn00Az4\n",
      "✅ Found: Speak No Evil → FjzxI6uf8H8\n",
      "✅ Found: Gun Monkeys → _lAdLC8JPXk\n",
      "✅ Found: Reality → plIUJ-LF7JU\n",
      "✅ Found: Another Woman (1988) → DM7eQBhYhxE\n",
      "✅ Found: USHER: Rendezvous in Paris → w0B9LkkJJb8\n",
      "✅ Found: Giant (re) → F9cX1v9zLdU\n",
      "✅ Found: Dead Poets Society → I0-K5zYQHD8\n",
      "✅ Found: Transformers One → u2NuUWuwPCM\n",
      "✅ Found: Never Let Go → ZDfRp_ukHDU\n",
      "✅ Found: Inland Empire (2006) (re) → nhJZpKRCdEQ\n",
      "✅ Found: Super/Man: The Christopher Reeve Story → gX-B3HMlMfY\n",
      "✅ Found: Lee → DmFYkiUAAA8\n",
      "✅ Found: Rob Peace → bugi-UdtjzI\n",
      "✅ Found: Joker: Folie à Deux → _OKAwz2MsJs\n",
      "✅ Found: Rope (re) → FaNa_OMhNn4\n",
      "❌ Error for 'Zabriskie Point': list index out of range\n",
      "✅ Found: Wild Robot, The → 67vbA5ZJdKQ\n",
      "✅ Found: Reagan → 7IcAsIvzSaM\n",
      "✅ Found: In a Violent Nature → WyXuRmXbS7U\n",
      "✅ Found: Funny Face (1957) → mQEHrJ0B8Cg\n",
      "✅ Found: Man Who Knew Too Much, The (1956) → EsEHdlDLlOM\n",
      "✅ Found: Love In The Afternoon → sxRMITE_2mY\n",
      "✅ Found: Something Wild → iXqagtRX4cU\n",
      "✅ Found: Private Hell 36 → ES5k5j1zzSc\n",
      "✅ Found: Smile 2 → 0HY6QFlBzUY\n",
      "✅ Found: Megalopolis → pq6mvHZU0fc\n",
      "✅ Found: Bigamist, The (1953) → TCCZcVp76N4\n",
      "✅ Found: Nam June Paik: Moon is the Oldest TV → ACjYjYgdbO0\n",
      "✅ Found: Venom: The Last Dance → __2bjWbetsA\n",
      "✅ Found: Goodrich → 7OlyW0AqDQo\n",
      "✅ Found: Anora → yWtzcjoUx7c\n",
      "✅ Found: Texas Chainsaw Massacre 50th Anniversary, The → MDUDUgU9G1I\n",
      "✅ Found: Rosemary's Baby (re: 2015) → V6wWKNij_1M\n",
      "✅ Found: Red One → U8XH3W0cMss\n",
      "✅ Found: Conclave → JX9jasdi3ic\n",
      "✅ Found: Terrifier 3 → zaPcin5knJk\n",
      "✅ Found: Here After → ukhI37_Z9XA\n",
      "✅ Found: Gladiator II → 4rgYUipGJNo\n",
      "✅ Found: Heretic → O9i2vmFhSSY\n",
      "✅ Found: Elevation → ZivlF_UYkN0\n",
      "✅ Found: Nuclear Now → bF0gWjlEpgo\n",
      "✅ Found: Placebo: This Search for Meaning → 9eFtsmydIyo\n",
      "✅ Found: Vidro Fumê → _Czwou6UfAA\n",
      "✅ Found: Platoon → Wd2366guKHw\n",
      "✅ Done scraping video IDs for batch 17.\n"
     ]
    }
   ],
   "source": [
    "###### batch 17\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the batch file\n",
    "df = pd.read_csv(\"batch_17_scrape.csv\")\n",
    "\n",
    "# Initialize YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # Replace with your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Distributor aliases\n",
    "distributor_aliases = {\n",
    "    \"fox searchlight\": \"searchlightpictures\",\n",
    "    \"searchlight\": \"searchlightpictures\",\n",
    "    \"20th century fox\": \"20thcenturystudios\",\n",
    "    \"20th century studios\": \"20thcenturystudios\",\n",
    "    \"sony\": \"sonypictures\",\n",
    "    \"sony int'l\": \"sonypictures\",\n",
    "    \"sony independent releasing\": \"sonypictures\",\n",
    "    \"sony pictures classics\": \"sonypicturesclassics\",\n",
    "    \"universal\": \"universalpictures\",\n",
    "    \"warner bros\": \"warnerbros\",\n",
    "    \"disney\": \"disneymovie\",\n",
    "    \"walt disney int'l\": \"disneymovie\",\n",
    "    \"a24\": \"a24films\",\n",
    "    \"lionsgate\": \"lionsgate\",\n",
    "    \"paramount\": \"paramountpictures\",\n",
    "    \"amazon studios\": \"amazonstudios\",\n",
    "    \"netflix\": \"netflix\",\n",
    "    \"mgm\": \"mgmstudios\",\n",
    "    \"focus features\": \"focusfeatures\",\n",
    "    \"neon rated\": \"neonrated\",\n",
    "    \"bleecker street\": \"bleeckerstreetfilms\",\n",
    "    \"ifc films\": \"ifcfilms\",\n",
    "    \"independent\": \"\",\n",
    "    \"unknown\": \"\"\n",
    "}\n",
    "\n",
    "# Function to get video ID\n",
    "def get_video_id(row):\n",
    "    title = row['title']\n",
    "    release_year = pd.to_datetime(row['release_date']).year\n",
    "    distributor = str(row['us_distributor']).lower().strip() if pd.notnull(row['us_distributor']) else \"\"\n",
    "    channel_hint = distributor_aliases.get(distributor, \"\")\n",
    "\n",
    "    query = f\"{title} {release_year} official trailer {channel_hint}\".strip()\n",
    "\n",
    "    try:\n",
    "        response = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"snippet\",\n",
    "            maxResults=1,\n",
    "            type=\"video\",\n",
    "            videoDuration=\"short\",\n",
    "            order=\"relevance\"\n",
    "        ).execute()\n",
    "        video_id = response['items'][0]['id']['videoId']\n",
    "        print(f\"✅ Found: {title} → {video_id}\")\n",
    "        return video_id\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error for '{title}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Apply with sleep in loop\n",
    "video_ids = []\n",
    "for idx, row in df.iterrows():\n",
    "    video_id = get_video_id(row)\n",
    "    video_ids.append(video_id)\n",
    "    time.sleep(0.3)  # Pause to avoid quota spikes\n",
    "\n",
    "df['video_id'] = video_ids\n",
    "\n",
    "# Save output\n",
    "df.to_csv(\"batch_17_with_ids.csv\", index=False)\n",
    "print(\"✅ Done scraping video IDs for batch 17.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b0ea489-267d-4fd3-a87b-dbc1583cfe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load batch with video IDs\n",
    "df = pd.read_csv(\"batch_17_with_ids.csv\")\n",
    "\n",
    "# Convert release_date column to datetime\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb14b6c8-cdbb-45a2-8aec-37a92f76093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\" \n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54d2a70f-fb57-46dc-af4f-bace7969c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve comments \n",
    "def scrape_comments(video_id, release_date, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_date = pd.to_datetime(snippet['publishedAt']).date()\n",
    "\n",
    "            if comment_date < release_date.date():\n",
    "                try:\n",
    "                    if detect(snippet['textDisplay']) == \"en\":\n",
    "                        comments.append({\n",
    "                            \"title\": title,\n",
    "                            \"date\": comment_date,\n",
    "                            \"author\": snippet['authorDisplayName'],\n",
    "                            \"comment\": snippet['textDisplay'],\n",
    "                            \"likes\": snippet['likeCount'],\n",
    "                            \"video_id\": video_id,\n",
    "                            \"release_date\": release_date\n",
    "                        })\n",
    "                except LangDetectException:\n",
    "                    continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ff7a6a-9459-4c14-8f91-d7ac901ff4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Land of Bad\n",
      "Scraping: Daddio\n",
      "Scraping: East of Eden\n",
      "Scraping: Rebel Without A Cause (1955)\n",
      "Scraping: Duck Soup\n",
      "Scraping: Ruggles of Red Gap (re)\n",
      "Scraping: Speak No Evil\n",
      "Scraping: Gun Monkeys\n",
      "Scraping: Reality\n",
      "Scraping: Another Woman (1988)\n",
      "Scraping: USHER: Rendezvous in Paris\n",
      "Scraping: Giant (re)\n",
      "Scraping: Dead Poets Society\n",
      "Scraping: Transformers One\n",
      "Scraping: Never Let Go\n",
      "Scraping: Inland Empire (2006) (re)\n",
      "Scraping: Super/Man: The Christopher Reeve Story\n",
      "Scraping: Lee\n",
      "Scraping: Rob Peace\n",
      "Scraping: Joker: Folie à Deux\n",
      "Scraping: Rope (re)\n",
      "Scraping: Wild Robot, The\n",
      "Scraping: Reagan\n",
      "Scraping: In a Violent Nature\n",
      "Scraping: Funny Face (1957)\n",
      "Scraping: Man Who Knew Too Much, The (1956)\n",
      "Scraping: Love In The Afternoon\n",
      "Scraping: Something Wild\n",
      "Scraping: Private Hell 36\n",
      "Scraping: Smile 2\n",
      "Scraping: Megalopolis\n",
      "Scraping: Bigamist, The (1953)\n",
      "Scraping: Nam June Paik: Moon is the Oldest TV\n",
      "Scraping: Venom: The Last Dance\n",
      "Scraping: Goodrich\n",
      "Scraping: Anora\n",
      "Scraping: Texas Chainsaw Massacre 50th Anniversary, The\n",
      "Scraping: Rosemary's Baby (re: 2015)\n",
      "Scraping: Red One\n",
      "Scraping: Conclave\n",
      "Scraping: Terrifier 3\n",
      "Scraping: Here After\n",
      "Scraping: Gladiator II\n",
      "Scraping: Heretic\n",
      "Scraping: Elevation\n",
      "Scraping: Nuclear Now\n",
      "Scraping: Placebo: This Search for Meaning\n",
      "Scraping: Vidro Fumê\n",
      "Scraping: Platoon\n"
     ]
    }
   ],
   "source": [
    "all_comments_batch17 = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['release_date'], row['title'])\n",
    "        all_comments_batch17.extend(comments)\n",
    "        time.sleep(0.5)  # reduce risk of quota issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f3065d3-3ccc-4c3e-92ad-b9bed43963ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Scraped 90278 comments for batch 17.\n"
     ]
    }
   ],
   "source": [
    "comments_df_17 = pd.DataFrame(all_comments_batch17)\n",
    "comments_df_17.to_csv(\"comments_batch_17.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df_17)} comments for batch 17.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa2b54f-6a46-45ab-86aa-9fcb5b5a35f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Vox Lux\n",
      "Scraping: Destroyer\n",
      "Scraping: Happy Death Day 2U\n",
      "Scraping: Lego Movie 2: The Second Part, The\n",
      "Scraping: Backtrace\n",
      "Scraping: Captain Marvel\n",
      "Scraping: Drunk Parents\n",
      "Scraping: Destination Wedding\n",
      "Scraping: After (2019)\n",
      "Scraping: Missing Link\n",
      "Scraping: Avengers: Endgame\n",
      "Scraping: We Die Young\n",
      "Scraping: Godzilla: King Of The Monsters (2019)\n",
      "Scraping: Haunting Of Sharon Tate, The\n",
      "Scraping: Don't Let Go\n",
      "Scraping: Escape Plan 3: The Extractors\n",
      "Scraping: Her Smell\n",
      "Scraping: Kitchen, The (2019)\n",
      "❌ Error fetching comments for Kitchen, The (2019): <HttpError 404 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=fgit74aVAvM%26t%3D1s&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter could not be found.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter could not be found.', 'domain': 'youtube.commentThread', 'reason': 'videoNotFound', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: Stockholm (2019)\n",
      "Scraping: Night Hunter\n",
      "Scraping: 47 Meters Down: Uncaged\n",
      "Scraping: Killerman\n",
      "Scraping: Playmobil: The Movie\n",
      "Scraping: Skin\n",
      "Scraping: Current War, The\n",
      "Scraping: Maleficent: Mistress Of Evil\n",
      "Scraping: Mutant Blast\n",
      "Scraping: Doctor Sleep\n",
      "Scraping: Midway\n",
      "Scraping: 21 Bridges\n",
      "Scraping: Star Wars: The Rise Of Skywalker\n",
      "Scraping: Spies In Disguise\n",
      "Scraping: Jexi\n",
      "Scraping: Wedding Year, The\n",
      "Scraping: Bigger\n",
      "Scraping: Gentlemen, The\n",
      "Scraping: Brahms: The Boy II\n",
      "Scraping: Invisible Man, The (2020)\n",
      "Scraping: Wolf Hour, The\n",
      "Scraping: Give Me Liberty\n",
      "Scraping: Burden (2020)\n",
      "Scraping: Ophelia\n",
      "Scraping: Tenet\n",
      "Scraping: After We Collided\n",
      "Scraping: Unhinged\n",
      "Scraping: Greenland\n",
      "Scraping: Trolls World Tour\n",
      "❌ Error fetching comments for Trolls World Tour: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=4_DZX7fJ6Yo&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: Follow Me\n",
      "✅ Done! Scraped 322321 comments for 2019–2020 corrected dataset.\n"
     ]
    }
   ],
   "source": [
    "###### after data cleaning and manual inspection, the following scrapes are for titles that were either missed in previous scrapes \n",
    "# or are corrected video IDs because previously for some titles, comments were scraped from unofficial trailers \n",
    "\n",
    "###### No more video ID retrieval here because video IDs have been manually inspected and assigned already\n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the cleaned file with only title and video_id\n",
    "df = pd.read_csv(\"corrected_ids_19_20.csv\")\n",
    "\n",
    "# Set up YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def scrape_comments(video_id, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching comments for {title}: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = snippet['textDisplay']\n",
    "\n",
    "            try:\n",
    "                if detect(comment_text) == \"en\":\n",
    "                    comments.append({\n",
    "                        \"title\": title,\n",
    "                        \"date\": snippet['publishedAt'],\n",
    "                        \"author\": snippet['authorDisplayName'],\n",
    "                        \"comment\": comment_text,\n",
    "                        \"likes\": snippet['likeCount'],\n",
    "                        \"video_id\": video_id\n",
    "                    })\n",
    "            except LangDetectException:\n",
    "                continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)  # throttle API calls\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Scrape all comments\n",
    "all_comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['title'])\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Save the results\n",
    "comments_df = pd.DataFrame(all_comments)\n",
    "comments_df.to_csv(\"comments_corrected_19_20.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df)} comments for 2019–2020 corrected dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b3348b-05f2-4708-865e-335aa90a404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Kitchen, The (2019)\n",
      "✅ Done! Scraped 792 comments for Kitchen, The (2019).\n"
     ]
    }
   ],
   "source": [
    "##### since comments for title Kitche, The (2019) could not be fetched in the last scrape, they have been scraped individually here \n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Setup YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# Define the title and video ID\n",
    "title = \"Kitchen, The (2019)\"\n",
    "video_id = \"fgit74aVAvM\"\n",
    "\n",
    "def scrape_comments(video_id, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching comments for {title}: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = snippet['textDisplay']\n",
    "\n",
    "            try:\n",
    "                if detect(comment_text) == \"en\":\n",
    "                    comments.append({\n",
    "                        \"title\": title,\n",
    "                        \"date\": snippet['publishedAt'],\n",
    "                        \"author\": snippet['authorDisplayName'],\n",
    "                        \"comment\": comment_text,\n",
    "                        \"likes\": snippet['likeCount'],\n",
    "                        \"video_id\": video_id\n",
    "                    })\n",
    "            except LangDetectException:\n",
    "                continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Run the scraper for this movie\n",
    "print(f\"Scraping: {title}\")\n",
    "comments = scrape_comments(video_id, title)\n",
    "\n",
    "# Save results\n",
    "comments_df = pd.DataFrame(comments)\n",
    "comments_df.to_csv(\"comments_kitchen_2019.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df)} comments for {title}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664ab27e-b36b-4e57-b647-569e413af408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Fatman\n",
      "Scraping: Music (Event)\n",
      "Scraping: Spiral (dir. Bousman)\n",
      "Scraping: Friendsgiving\n",
      "Scraping: Nobody\n",
      "Scraping: Synchronic\n",
      "Scraping: Marksman, The\n",
      "Scraping: F9 The Fast Saga\n",
      "Scraping: Black Widow\n",
      "Scraping: Gunpowder Milkshake\n",
      "Scraping: Suicide Squad, The\n",
      "Scraping: To the Stars (dir. Stephens)\n",
      "Scraping: Echo Boomers\n",
      "Scraping: Barb And Star Go To Vista del Mar\n",
      "Scraping: After We Fell (Event)\n",
      "Scraping: What Is Life Worth\n",
      "Scraping: Copshop\n",
      "Scraping: Eternals\n",
      "Scraping: Out of Death\n",
      "Scraping: 13 Minutes\n",
      "❌ Error fetching comments for 13 Minutes: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=11eHcVi9-v8&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: Swallow\n",
      "Scraping: Demonic\n",
      "Scraping: Doctor Strange in the Multiverse of Madness\n",
      "Scraping: Last Looks\n",
      "Scraping: Minions: The Rise Of Gru\n",
      "❌ Error fetching comments for Minions: The Rise Of Gru: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=6DxjJzmYsXo&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: Last Seen Alive\n",
      "Scraping: Thor: Love And Thunder\n",
      "Scraping: Book of Love\n",
      "Scraping: Blacklight\n",
      "Scraping: Don't Worry Darling\n",
      "Scraping: The Exorcism of God\n",
      "Scraping: Black Panther: Wakanda Forever\n",
      "Scraping: Puss In Boots: The Last Wish\n",
      "❌ Error fetching comments for Puss In Boots: The Last Wish: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=xgZLXyqbYOc&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: Rock Dog 3 Battle the Beat\n",
      "❌ Error fetching comments for Rock Dog 3 Battle the Beat: <HttpError 404 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=tFmyP-nz7M&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter could not be found.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter could not be found.', 'domain': 'youtube.commentThread', 'reason': 'videoNotFound', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "✅ Done! Scraped 200780 comments for 2021–2022 corrected dataset.\n"
     ]
    }
   ],
   "source": [
    "##### more scraping of inspected and corrected video IDs \n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the cleaned file with only title and video_id\n",
    "df = pd.read_csv(\"corrected_ids_21_22.csv\")\n",
    "\n",
    "# Set up YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def scrape_comments(video_id, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching comments for {title}: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = snippet['textDisplay']\n",
    "\n",
    "            try:\n",
    "                if detect(comment_text) == \"en\":\n",
    "                    comments.append({\n",
    "                        \"title\": title,\n",
    "                        \"date\": snippet['publishedAt'],\n",
    "                        \"author\": snippet['authorDisplayName'],\n",
    "                        \"comment\": comment_text,\n",
    "                        \"likes\": snippet['likeCount'],\n",
    "                        \"video_id\": video_id\n",
    "                    })\n",
    "            except LangDetectException:\n",
    "                continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)  # throttle API calls\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Scrape all comments\n",
    "all_comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['title'])\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Save the results\n",
    "comments_df = pd.DataFrame(all_comments)\n",
    "comments_df.to_csv(\"comments_corrected_21_22.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df)} comments for 2021–2022 corrected dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1130ff84-e3d2-46d2-add7-7ac9fce277b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Fatman\n",
      "Scraping: Music (Event)\n",
      "Scraping: Spiral (dir. Bousman)\n",
      "Scraping: Friendsgiving\n",
      "Scraping: Nobody\n",
      "Scraping: Synchronic\n",
      "Scraping: Marksman, The\n",
      "Scraping: F9 The Fast Saga\n",
      "Scraping: Black Widow\n",
      "Scraping: Gunpowder Milkshake\n",
      "Scraping: Suicide Squad, The\n",
      "Scraping: To the Stars (dir. Stephens)\n",
      "Scraping: Echo Boomers\n",
      "Scraping: Barb And Star Go To Vista del Mar\n",
      "Scraping: After We Fell (Event)\n",
      "Scraping: What Is Life Worth\n",
      "Scraping: Copshop\n",
      "Scraping: Eternals\n",
      "Scraping: Out of Death\n",
      "Scraping: 13 Minutes\n",
      "❌ Error fetching comments for 13 Minutes: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=11eHcVi9-v8&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: Swallow\n",
      "Scraping: Demonic\n",
      "Scraping: Doctor Strange in the Multiverse of Madness\n",
      "Scraping: Last Looks\n",
      "Scraping: Minions: The Rise Of Gru\n",
      "❌ Error fetching comments for Minions: The Rise Of Gru: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=6DxjJzmYsXo&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: Last Seen Alive\n",
      "Scraping: Thor: Love And Thunder\n",
      "Scraping: Book of Love\n",
      "Scraping: Blacklight\n",
      "Scraping: Don't Worry Darling\n",
      "Scraping: The Exorcism of God\n",
      "Scraping: Black Panther: Wakanda Forever\n",
      "Scraping: Puss In Boots: The Last Wish\n",
      "❌ Error fetching comments for Puss In Boots: The Last Wish: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=xgZLXyqbYOc&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: Rock Dog 3 Battle the Beat\n",
      "❌ Error fetching comments for Rock Dog 3 Battle the Beat: <HttpError 404 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=tFmyP-nz7M&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter could not be found.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter could not be found.', 'domain': 'youtube.commentThread', 'reason': 'videoNotFound', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "✅ Done! Scraped 200780 comments for 2021–2022 corrected dataset.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the cleaned file with only title and video_id\n",
    "df = pd.read_csv(\"corrected_ids_21_22.csv\")\n",
    "\n",
    "# Set up YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def scrape_comments(video_id, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching comments for {title}: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = snippet['textDisplay']\n",
    "\n",
    "            try:\n",
    "                if detect(comment_text) == \"en\":\n",
    "                    comments.append({\n",
    "                        \"title\": title,\n",
    "                        \"date\": snippet['publishedAt'],\n",
    "                        \"author\": snippet['authorDisplayName'],\n",
    "                        \"comment\": comment_text,\n",
    "                        \"likes\": snippet['likeCount'],\n",
    "                        \"video_id\": video_id\n",
    "                    })\n",
    "            except LangDetectException:\n",
    "                continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)  # throttle API calls\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Scrape all comments\n",
    "all_comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['title'])\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Save the results\n",
    "comments_df = pd.DataFrame(all_comments)\n",
    "comments_df.to_csv(\"comments_corrected_21_22.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df)} comments for 2021–2022 corrected dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49339de7-0980-4810-84f9-06a9aa684812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Wrong Place\n",
      "Scraping: Inspection, The\n",
      "Scraping: Ant-Man and the Wasp: Quantumania\n",
      "Scraping: To Leslie\n",
      "Scraping: Whale, The\n",
      "Scraping: Savage Salvation\n",
      "Scraping: Till\n",
      "Scraping: White Elephant\n",
      "Scraping: On the Line (2022)\n",
      "Scraping: Guardians of the Galaxy Vol. 3\n",
      "Scraping: Fast X\n",
      "Scraping: Spider-Man: Across The Spider-Verse\n",
      "Scraping: The Offering\n",
      "Scraping: Never Rarely Sometimes Always\n",
      "Scraping: Indiana Jones and the Dial of Destiny\n",
      "Scraping: After Everything (Event)\n",
      "Scraping: Blood\n",
      "Scraping: To Catch a Killer\n",
      "Scraping: Perpetrator\n",
      "Scraping: She Came to Me\n",
      "Scraping: Marvels, The\n",
      "Scraping: Master Gardener\n",
      "Scraping: Wish\n",
      "Scraping: Hypnotic\n",
      "Scraping: Migration\n",
      "❌ Error fetching comments for Migration: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=cQfo0HJhCnE&maxResults=100&textFormat=plainText&key=AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Scraping: All Fun and Games\n",
      "Scraping: It's a Wonderful Knife\n",
      "Scraping: Anyone But You\n",
      "Scraping: Poor Things\n",
      "Scraping: Beautiful Wedding\n",
      "Scraping: On Fire\n",
      "Scraping: Bricklayer, The\n",
      "Scraping: Sleeping Dogs\n",
      "Scraping: Cabrini\n",
      "Scraping: Origin\n",
      "Scraping: Exorcism, The\n",
      "Scraping: Freud's Last Session\n",
      "Scraping: Memory\n",
      "Scraping: Deadpool & Wolverine\n",
      "Scraping: In the Land of Saints and Sinners\n",
      "Scraping: Play Dead\n",
      "Scraping: Fast Charlie\n",
      "Scraping: Reagan\n",
      "Scraping: Anora\n",
      "Scraping: Terrifier 3\n",
      "Scraping: Elevation\n",
      "Scraping: Poolman\n",
      "Scraping: Kraven the Hunter\n",
      "Scraping: Sasquatch Sunset\n",
      "Scraping: Babygirl\n",
      "✅ Done! Scraped 156909 comments for 2023–2024 corrected dataset.\n"
     ]
    }
   ],
   "source": [
    "#### more scraping of inspected and corrected video IDs \n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the cleaned file with only title and video_id\n",
    "df = pd.read_csv(\"corrected_ids_23_24.csv\")\n",
    "\n",
    "# Set up YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "def scrape_comments(video_id, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching comments for {title}: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = snippet['textDisplay']\n",
    "\n",
    "            try:\n",
    "                if detect(comment_text) == \"en\":\n",
    "                    comments.append({\n",
    "                        \"title\": title,\n",
    "                        \"date\": snippet['publishedAt'],\n",
    "                        \"author\": snippet['authorDisplayName'],\n",
    "                        \"comment\": comment_text,\n",
    "                        \"likes\": snippet['likeCount'],\n",
    "                        \"video_id\": video_id\n",
    "                    })\n",
    "            except LangDetectException:\n",
    "                continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)  # throttle API calls\n",
    "\n",
    "    return comments\n",
    "\n",
    "# Scrape all comments\n",
    "all_comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['title'])\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Save the results\n",
    "comments_df = pd.DataFrame(all_comments)\n",
    "comments_df.to_csv(\"comments_corrected_23_24.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df)} comments for 2023–2024 corrected dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0f305a-58ac-44f5-805d-0bbad28aee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: Serenity\n",
      "Scraping: Spider-Man: Far From Home\n",
      "Scraping: Outpost, The (2020)\n",
      "Scraping: Endless\n",
      "Scraping: Force Of Nature (2020)\n",
      "Scraping: Love, Weddings & Other Disasters\n",
      "Scraping: Raya And The Last Dragon\n",
      "Scraping: Rock Dog 2: Walk Around the Park\n",
      "Scraping: Smile (2022)\n",
      "Scraping: Operation Fortune: Ruse de Guerre\n",
      "Scraping: Killer, The (2023)\n",
      "Scraping: White Bird\n",
      "Scraping: Imaginary\n",
      "Scraping: Mothers' Instinct\n",
      "Scraping: Haunting of the Queen Mary\n",
      "Scraping: Crow, The\n",
      "Scraping: Beetlejuice Beetlejuice\n",
      "Scraping: Land of Bad\n",
      "Scraping: Daddio\n",
      "Scraping: Speak No Evil\n",
      "Scraping: Reality\n",
      "Scraping: Transformers One\n",
      "Scraping: Never Let Go\n",
      "Scraping: Lee\n",
      "Scraping: Rob Peace\n",
      "Scraping: Joker: Folie à Deux\n",
      "Scraping: Wild Robot, The\n",
      "Scraping: In a Violent Nature\n",
      "Scraping: Smile 2\n",
      "Scraping: Megalopolis\n",
      "Scraping: Venom: The Last Dance\n",
      "Scraping: Goodrich\n",
      "Scraping: Red One\n",
      "Scraping: Conclave\n",
      "Scraping: Here After\n",
      "Scraping: Gladiator II\n",
      "Scraping: Heretic\n",
      "Scraping: Bagman\n",
      "Scraping: Red Right Hand\n",
      "Scraping: Wicked\n",
      "Scraping: Lord of the Rings: The War of the Rohirrim\n",
      "Scraping: Sonic The Hedgehog 3\n",
      "Scraping: Here\n",
      "Scraping: Ezra\n",
      "Scraping: Mufasa: The Lion King\n",
      "✅ Done! Scraped 235484 comments for final scrape titles dataset.\n"
     ]
    }
   ],
   "source": [
    "###### scraping of some titles that were missed in previous scrapes; and this is where the comments ends  \n",
    "import pandas as pd\n",
    "import time\n",
    "from langdetect import detect, LangDetectException\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "# Set working directory if needed\n",
    "os.chdir(\"/Users/talhaahmed/Desktop/Thesis/Thesis Data\")\n",
    "\n",
    "# Load the cleaned file with only title and video_id\n",
    "df = pd.read_csv(\"final_scrape_titles_with_ids.csv\")\n",
    "\n",
    "# Set up YouTube API\n",
    "API_KEY = \"AIzaSyB9rHTNuN5_npfrI7d00W7kvvohJ3eH7Zs\"  # your actual API key\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "def scrape_comments(video_id, title):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            )\n",
    "            response = request.execute()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching comments for {title}: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = snippet['textDisplay']\n",
    "\n",
    "            try:\n",
    "                if detect(comment_text) == \"en\":\n",
    "                    comments.append({\n",
    "                        \"title\": title,\n",
    "                        \"date\": snippet['publishedAt'],\n",
    "                        \"author\": snippet['authorDisplayName'],\n",
    "                        \"comment\": comment_text,\n",
    "                        \"likes\": snippet['likeCount'],\n",
    "                        \"video_id\": video_id\n",
    "                    })\n",
    "            except LangDetectException:\n",
    "                continue\n",
    "\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "        time.sleep(0.3)  # throttle API calls\n",
    "    return comments\n",
    "\n",
    "# Scrape all comments\n",
    "all_comments = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row['video_id']):\n",
    "        print(f\"Scraping: {row['title']}\")\n",
    "        comments = scrape_comments(row['video_id'], row['title'])\n",
    "        all_comments.extend(comments)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Save the results\n",
    "comments_df = pd.DataFrame(all_comments)\n",
    "comments_df.to_csv(\"comments_final_scrape_titles.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Scraped {len(comments_df)} comments for final scrape titles dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
